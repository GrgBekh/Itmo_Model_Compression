{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5fddbf4a553b4105bdd82ab77aefd52e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80543701f9ac48d281ac264bb58c344a",
              "IPY_MODEL_99dd1e9127514c80b232dd41af81cfb4",
              "IPY_MODEL_485b5f3248e148b3b460244a3ebf1143"
            ],
            "layout": "IPY_MODEL_a884814dfe1d4261903817798702a4d7"
          }
        },
        "80543701f9ac48d281ac264bb58c344a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a04b23a5423e4059a0d97ff583607a32",
            "placeholder": "​",
            "style": "IPY_MODEL_bbb61f55e5664097bbbfe78cc37a222c",
            "value": "100%"
          }
        },
        "99dd1e9127514c80b232dd41af81cfb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_210f264b788a4724b6fb6a70d805c30e",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c44615fa3db4767ae94e470afc6b177",
            "value": 100
          }
        },
        "485b5f3248e148b3b460244a3ebf1143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2328b8542b648539497a53fedbc864d",
            "placeholder": "​",
            "style": "IPY_MODEL_a0629994ed15450e95f76e2552d49fd0",
            "value": " 100/100 [00:08&lt;00:00,  7.37it/s]"
          }
        },
        "a884814dfe1d4261903817798702a4d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a04b23a5423e4059a0d97ff583607a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbb61f55e5664097bbbfe78cc37a222c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "210f264b788a4724b6fb6a70d805c30e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c44615fa3db4767ae94e470afc6b177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2328b8542b648539497a53fedbc864d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0629994ed15450e95f76e2552d49fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2334f84947f4cf289c22c8150febf40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0aa24e7ba3df46aa95ccb588a9e234fc",
              "IPY_MODEL_9d4231b0115649a6924e5f681dc3b7c4",
              "IPY_MODEL_0001e2b7debb4bdebd5c09ddd5a4333f"
            ],
            "layout": "IPY_MODEL_8c8adcd231eb4b1ea82efcd3c42ddae3"
          }
        },
        "0aa24e7ba3df46aa95ccb588a9e234fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_925c28d560734ddab1da2378d78d3f4b",
            "placeholder": "​",
            "style": "IPY_MODEL_02852087b7464b97bc3e46d35f10d948",
            "value": "100%"
          }
        },
        "9d4231b0115649a6924e5f681dc3b7c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b1f90d886ac49e086d68d184e6ccf8b",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f1dc81f3ab44193bfad05c910c73e0c",
            "value": 100
          }
        },
        "0001e2b7debb4bdebd5c09ddd5a4333f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9579ee28938440fe9635bb2a5bdcc844",
            "placeholder": "​",
            "style": "IPY_MODEL_82be1a008dba4f05b45545b5df0cee37",
            "value": " 100/100 [00:06&lt;00:00, 13.44it/s]"
          }
        },
        "8c8adcd231eb4b1ea82efcd3c42ddae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "925c28d560734ddab1da2378d78d3f4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02852087b7464b97bc3e46d35f10d948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b1f90d886ac49e086d68d184e6ccf8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f1dc81f3ab44193bfad05c910c73e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9579ee28938440fe9635bb2a5bdcc844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82be1a008dba4f05b45545b5df0cee37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c47c70320a65487db3819c445336b1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ac30dd0b0f3436ca22111620247d2a3",
              "IPY_MODEL_2c8e77c01eba4fc8a52e4b65a5dc868f",
              "IPY_MODEL_4e2668d003d14df0a7907f616785f66b"
            ],
            "layout": "IPY_MODEL_a2e048953ddb459daeeab913c86c38d8"
          }
        },
        "9ac30dd0b0f3436ca22111620247d2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3bc08d2f1cf4dad9323b67fb461e6ff",
            "placeholder": "​",
            "style": "IPY_MODEL_b94551a2b29d471e9e3b92dfd221bfbd",
            "value": "100%"
          }
        },
        "2c8e77c01eba4fc8a52e4b65a5dc868f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d56695cd421b4c218eb151a8be19fc80",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f60fd9ae6134e8c856876d1bce53fcd",
            "value": 100
          }
        },
        "4e2668d003d14df0a7907f616785f66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1482ba59c7024fecb8b5b6353bbe210c",
            "placeholder": "​",
            "style": "IPY_MODEL_e7912bd1137f409fba47ecec7ca027e6",
            "value": " 100/100 [00:07&lt;00:00, 16.17it/s]"
          }
        },
        "a2e048953ddb459daeeab913c86c38d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3bc08d2f1cf4dad9323b67fb461e6ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b94551a2b29d471e9e3b92dfd221bfbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d56695cd421b4c218eb151a8be19fc80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f60fd9ae6134e8c856876d1bce53fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1482ba59c7024fecb8b5b6353bbe210c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7912bd1137f409fba47ecec7ca027e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe0e8GY6eCFY",
        "outputId": "0784a8fe-404e-4ded-896d-684ede3ab380"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.181-py3-none-any.whl (617 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.1/617.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (16.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: ultralytics\n",
            "Successfully installed ultralytics-8.0.181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "Wdftgk5teJaR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WeciXJ7luC-0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import models\n",
        "from torch.nn.utils import prune\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "qSzs_fbPt6I5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "E8pPy3xW-_ax"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8n.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmTPgMIxeOK3",
        "outputId": "a8014420-b08b-4f28-b74d-552cb08e69e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100%|██████████| 6.23M/6.23M [00:00<00:00, 11.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"%.2f MB\" %(os.path.getsize(\"yolov8n.pt\")/1e6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52SeCz-Hw4eJ",
        "outputId": "977d5885-7a70-4b37-b354-8448292885db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.53 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J5RFLNc_JTi",
        "outputId": "e1f6bdb8-9531-4a02-ff43-c2173b3cda26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGKXt-yEBGLt",
        "outputId": "b49541e4-82d3-40fd-f65f-3b8464f77986"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 12.085MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "num_samples = 100\n",
        "start_time = time.time()\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    output = model(inp / 255)\n",
        "end_time = time.time()\n",
        "\n",
        "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
        "print(f'Avg inference time: {infer_time:.4f} sec')"
      ],
      "metadata": {
        "id": "xPh-Kyu9_ssN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5fddbf4a553b4105bdd82ab77aefd52e",
            "80543701f9ac48d281ac264bb58c344a",
            "99dd1e9127514c80b232dd41af81cfb4",
            "485b5f3248e148b3b460244a3ebf1143",
            "a884814dfe1d4261903817798702a4d7",
            "a04b23a5423e4059a0d97ff583607a32",
            "bbb61f55e5664097bbbfe78cc37a222c",
            "210f264b788a4724b6fb6a70d805c30e",
            "6c44615fa3db4767ae94e470afc6b177",
            "c2328b8542b648539497a53fedbc864d",
            "a0629994ed15450e95f76e2552d49fd0"
          ]
        },
        "outputId": "41c28702-79ca-4145-c746-4facf505ffd3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fddbf4a553b4105bdd82ab77aefd52e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 224x224 (no detections), 219.3ms\n",
            "Speed: 0.0ms preprocess, 219.3ms inference, 12.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 69.1ms\n",
            "Speed: 0.0ms preprocess, 69.1ms inference, 1.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 63.8ms\n",
            "Speed: 0.0ms preprocess, 63.8ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 78.4ms\n",
            "Speed: 0.0ms preprocess, 78.4ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.0ms\n",
            "Speed: 0.0ms preprocess, 46.0ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.4ms\n",
            "Speed: 0.0ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.4ms\n",
            "Speed: 0.0ms preprocess, 49.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 40.1ms\n",
            "Speed: 0.0ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.5ms\n",
            "Speed: 0.0ms preprocess, 39.5ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.8ms\n",
            "Speed: 0.0ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 48.3ms\n",
            "Speed: 0.0ms preprocess, 48.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.8ms\n",
            "Speed: 0.0ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 44.5ms\n",
            "Speed: 0.0ms preprocess, 44.5ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 48.6ms\n",
            "Speed: 0.0ms preprocess, 48.6ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 52.5ms\n",
            "Speed: 0.0ms preprocess, 52.5ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 41.8ms\n",
            "Speed: 0.0ms preprocess, 41.8ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.0ms\n",
            "Speed: 0.0ms preprocess, 49.0ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.5ms\n",
            "Speed: 0.0ms preprocess, 49.5ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 44.9ms\n",
            "Speed: 0.0ms preprocess, 44.9ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 40.6ms\n",
            "Speed: 0.0ms preprocess, 40.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 42.2ms\n",
            "Speed: 0.0ms preprocess, 42.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 60.2ms\n",
            "Speed: 0.0ms preprocess, 60.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.3ms\n",
            "Speed: 0.0ms preprocess, 45.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.6ms\n",
            "Speed: 0.0ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.6ms\n",
            "Speed: 0.0ms preprocess, 45.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 41.1ms\n",
            "Speed: 0.0ms preprocess, 41.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.8ms\n",
            "Speed: 0.0ms preprocess, 45.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.4ms\n",
            "Speed: 0.0ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 44.0ms\n",
            "Speed: 0.0ms preprocess, 44.0ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.1ms\n",
            "Speed: 0.0ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 48.5ms\n",
            "Speed: 0.0ms preprocess, 48.5ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 38.4ms\n",
            "Speed: 0.0ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 47.3ms\n",
            "Speed: 0.0ms preprocess, 47.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.2ms\n",
            "Speed: 0.0ms preprocess, 46.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 41.0ms\n",
            "Speed: 0.0ms preprocess, 41.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.0ms\n",
            "Speed: 0.0ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 56.2ms\n",
            "Speed: 0.0ms preprocess, 56.2ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 43.4ms\n",
            "Speed: 0.0ms preprocess, 43.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.3ms\n",
            "Speed: 0.0ms preprocess, 46.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.2ms\n",
            "Speed: 0.0ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 59.6ms\n",
            "Speed: 0.0ms preprocess, 59.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.6ms\n",
            "Speed: 0.0ms preprocess, 45.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 42.8ms\n",
            "Speed: 0.0ms preprocess, 42.8ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.4ms\n",
            "Speed: 0.0ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 44.3ms\n",
            "Speed: 0.0ms preprocess, 44.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 41.2ms\n",
            "Speed: 0.0ms preprocess, 41.2ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 43.9ms\n",
            "Speed: 0.0ms preprocess, 43.9ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 38.4ms\n",
            "Speed: 0.0ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.6ms\n",
            "Speed: 0.0ms preprocess, 45.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.7ms\n",
            "Speed: 0.0ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 50.2ms\n",
            "Speed: 0.0ms preprocess, 50.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 47.0ms\n",
            "Speed: 0.0ms preprocess, 47.0ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 47.2ms\n",
            "Speed: 0.0ms preprocess, 47.2ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 52.8ms\n",
            "Speed: 0.0ms preprocess, 52.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 44.1ms\n",
            "Speed: 0.0ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.2ms\n",
            "Speed: 0.0ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 48.6ms\n",
            "Speed: 0.0ms preprocess, 48.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.7ms\n",
            "Speed: 0.0ms preprocess, 49.7ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 44.5ms\n",
            "Speed: 0.0ms preprocess, 44.5ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.2ms\n",
            "Speed: 0.0ms preprocess, 49.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.8ms\n",
            "Speed: 0.0ms preprocess, 46.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 39.3ms\n",
            "Speed: 0.0ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.6ms\n",
            "Speed: 0.0ms preprocess, 45.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 41.3ms\n",
            "Speed: 0.0ms preprocess, 41.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 41.9ms\n",
            "Speed: 0.0ms preprocess, 41.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 40.0ms\n",
            "Speed: 0.0ms preprocess, 40.0ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 43.1ms\n",
            "Speed: 0.0ms preprocess, 43.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 42.5ms\n",
            "Speed: 0.0ms preprocess, 42.5ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.2ms\n",
            "Speed: 0.0ms preprocess, 46.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 41.0ms\n",
            "Speed: 0.0ms preprocess, 41.0ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 50.5ms\n",
            "Speed: 0.0ms preprocess, 50.5ms inference, 1.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.3ms\n",
            "Speed: 0.0ms preprocess, 45.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 41.8ms\n",
            "Speed: 0.0ms preprocess, 41.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 41.3ms\n",
            "Speed: 0.0ms preprocess, 41.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.7ms\n",
            "Speed: 0.0ms preprocess, 49.7ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 38.8ms\n",
            "Speed: 0.0ms preprocess, 38.8ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.1ms\n",
            "Speed: 0.0ms preprocess, 46.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 44.0ms\n",
            "Speed: 0.0ms preprocess, 44.0ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 47.4ms\n",
            "Speed: 0.0ms preprocess, 47.4ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 37.9ms\n",
            "Speed: 0.1ms preprocess, 37.9ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 43.6ms\n",
            "Speed: 0.0ms preprocess, 43.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 44.4ms\n",
            "Speed: 0.0ms preprocess, 44.4ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 40.9ms\n",
            "Speed: 0.0ms preprocess, 40.9ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 42.7ms\n",
            "Speed: 0.0ms preprocess, 42.7ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 47.3ms\n",
            "Speed: 0.0ms preprocess, 47.3ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 44.5ms\n",
            "Speed: 0.0ms preprocess, 44.5ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.1ms\n",
            "Speed: 0.0ms preprocess, 45.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 61.0ms\n",
            "Speed: 0.0ms preprocess, 61.0ms inference, 9.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 114.1ms\n",
            "Speed: 0.0ms preprocess, 114.1ms inference, 2.4ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 84.1ms\n",
            "Speed: 0.0ms preprocess, 84.1ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 60.5ms\n",
            "Speed: 0.0ms preprocess, 60.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 64.1ms\n",
            "Speed: 0.0ms preprocess, 64.1ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 90.8ms\n",
            "Speed: 0.0ms preprocess, 90.8ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 92.3ms\n",
            "Speed: 0.0ms preprocess, 92.3ms inference, 1.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 128.7ms\n",
            "Speed: 0.0ms preprocess, 128.7ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 99.8ms\n",
            "Speed: 0.0ms preprocess, 99.8ms inference, 56.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 168.3ms\n",
            "Speed: 0.0ms preprocess, 168.3ms inference, 7.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 197.2ms\n",
            "Speed: 0.0ms preprocess, 197.2ms inference, 12.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 87.1ms\n",
            "Speed: 0.0ms preprocess, 87.1ms inference, 1.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 98.5ms\n",
            "Speed: 0.0ms preprocess, 98.5ms inference, 1.6ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg inference time: 85.0392 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
        "print(f'Avg inference time: {infer_time:.4f} ms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efmh803MBgxe",
        "outputId": "d3c642b9-b16f-4b5c-c513-c708d686dd76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg inference time: 85.0392 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57AnOq0ft-Bs",
        "outputId": "f083fef5-849e-4f2c-bce9-7faf8fcc2db6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO(\n",
            "  (model): DetectionModel(\n",
            "    (model): Sequential(\n",
            "      (0): Conv(\n",
            "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (1): Conv(\n",
            "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (2): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): Conv(\n",
            "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (4): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-1): 2 x Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (5): Conv(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (6): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-1): 2 x Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (7): Conv(\n",
            "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (8): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (9): SPPF(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
            "      (11): Concat()\n",
            "      (12): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
            "      (14): Concat()\n",
            "      (15): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (16): Conv(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (17): Concat()\n",
            "      (18): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (19): Conv(\n",
            "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (20): Concat()\n",
            "      (21): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (22): Detect(\n",
            "        (cv2): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (cv3): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (dfl): DFL(\n",
            "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model,\n",
        "    {torch.nn.Conv2d},\n",
        "    dtype=torch.qint8\n",
        ")"
      ],
      "metadata": {
        "id": "zgffMzZpCCLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2baa07-6c6f-4635-893f-2b1f239193b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.181 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco8.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "\n",
            "Dataset 'coco8.yaml' images not found ⚠️, missing path '/content/datasets/coco8/images/val'\n",
            "Downloading https://ultralytics.com/assets/coco8.zip to '/content/datasets/coco8.zip'...\n",
            "100%|██████████| 433k/433k [00:00<00:00, 3.97MB/s]\n",
            "Unzipping /content/datasets/coco8.zip to /content/datasets/coco8...: 100%|██████████| 25/25 [00:00<00:00, 2441.33file/s]\n",
            "Dataset download success ✅ (1.5s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100%|██████████| 755k/755k [00:00<00:00, 5.11MB/s]\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
            "\n",
            "Transferred 70/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8/labels/train... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 168.38it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco8/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 6713.57it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8/labels/val.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/100         0G      2.637       5.14      2.668         37        640: 100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
            "                   all          4         17      0.168     0.0167   0.000565    0.00019\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      2/100         0G       2.74      5.343      2.662         39        640: 100%|██████████| 1/1 [00:03<00:00,  3.22s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
            "                   all          4         17     0.0279     0.0167    0.00404    0.00161\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      3/100         0G      2.623      5.888      2.767         18        640: 100%|██████████| 1/1 [00:04<00:00,  4.17s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
            "                   all          4         17   0.000165     0.0167   0.000301   0.000211\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      4/100         0G      2.868      5.202      2.777         43        640: 100%|██████████| 1/1 [00:04<00:00,  4.90s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n",
            "                   all          4         17      0.168     0.0333    0.00102   0.000256\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      5/100         0G      2.827      5.799      2.771         18        640: 100%|██████████| 1/1 [00:08<00:00,  8.02s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n",
            "                   all          4         17    0.00241       0.05    0.00145   0.000441\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      6/100         0G      2.522      5.068      2.603         41        640: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
            "                   all          4         17   0.000831       0.05   0.000917   0.000274\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      7/100         0G      3.619      5.972      3.199         19        640: 100%|██████████| 1/1 [00:03<00:00,  3.42s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
            "                   all          4         17    0.00065     0.0333   0.000845    0.00026\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      8/100         0G      2.434      5.574      2.555         23        640: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000293     0.0333   0.000752   0.000144\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      9/100         0G      2.512      5.244      2.617         36        640: 100%|██████████| 1/1 [00:04<00:00,  4.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
            "                   all          4         17   0.000144     0.0167   0.000317   3.17e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     10/100         0G      3.142       6.12      2.984         16        640: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000144     0.0167   0.000699    0.00014\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     11/100         0G      3.164      6.481      3.012         13        640: 100%|██████████| 1/1 [00:03<00:00,  3.04s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
            "                   all          4         17   0.000287     0.0333    0.00118   0.000118\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     12/100         0G      2.709       5.23      2.665         36        640: 100%|██████████| 1/1 [00:03<00:00,  3.73s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000285     0.0333    0.00178   0.000206\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     13/100         0G      2.775      5.343      2.795         31        640: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000423       0.05    0.00277    0.00029\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     14/100         0G      2.543      5.328      2.604         33        640: 100%|██████████| 1/1 [00:03<00:00,  3.77s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
            "                   all          4         17   0.000422       0.05    0.00285   0.000299\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     15/100         0G      3.298      7.146      2.883         11        640: 100%|██████████| 1/1 [00:03<00:00,  3.38s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17    0.00042       0.05    0.00329   0.000343\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     16/100         0G      2.844      5.318      2.832         31        640: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17    0.00268     0.0333    0.00496   0.000511\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     17/100         0G      2.701      5.177      2.667         38        640: 100%|██████████| 1/1 [00:03<00:00,  3.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17     0.0167     0.0167    0.00888    0.00171\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     18/100         0G      3.221      5.553      2.845         19        640: 100%|██████████| 1/1 [00:03<00:00,  3.00s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17     0.0167     0.0167    0.00888    0.00171\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     19/100         0G      2.715      5.631      2.647         24        640: 100%|██████████| 1/1 [00:03<00:00,  3.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
            "                   all          4         17     0.0106     0.0167    0.00884     0.0025\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     20/100         0G      2.595      5.345       2.67         28        640: 100%|██████████| 1/1 [00:03<00:00,  3.04s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17     0.0106     0.0167    0.00884     0.0025\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     21/100         0G      2.361      5.399      2.489         23        640: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 0.700s exceeded\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n",
            "                   all          4         17   0.000371     0.0333    0.00042   6.69e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     22/100         0G      2.987      5.563      2.696         32        640: 100%|██████████| 1/1 [00:03<00:00,  3.50s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17    0.00173     0.0333    0.00835    0.00405\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     23/100         0G      2.652      5.086      2.582         41        640: 100%|██████████| 1/1 [00:03<00:00,  3.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00839    0.00485\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     24/100         0G      2.724      5.397      2.822         29        640: 100%|██████████| 1/1 [00:04<00:00,  4.76s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00839    0.00485\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     25/100         0G      2.659      5.562      2.621         29        640: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00842    0.00483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     26/100         0G      2.407      5.185      2.438         32        640: 100%|██████████| 1/1 [00:03<00:00,  4.00s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00842    0.00483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     27/100         0G      2.658      5.379      2.703         32        640: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000418       0.05     0.0058    0.00272\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     28/100         0G      2.419      5.612      2.671         16        640: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000418       0.05     0.0058    0.00272\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     29/100         0G      2.758      6.035      2.875         15        640: 100%|██████████| 1/1 [00:04<00:00,  4.33s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00548    0.00268\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     30/100         0G       2.71      5.462      2.528         23        640: 100%|██████████| 1/1 [00:03<00:00,  3.02s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00548    0.00268\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     31/100         0G      2.262      5.346      2.366         31        640: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00416    0.00204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     32/100         0G      2.525      6.001      2.617         13        640: 100%|██████████| 1/1 [00:03<00:00,  3.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00416    0.00204\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     33/100         0G      2.507      5.018      2.466         43        640: 100%|██████████| 1/1 [00:03<00:00,  3.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00417    0.00244\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     34/100         0G      2.817      5.135      2.708         36        640: 100%|██████████| 1/1 [00:04<00:00,  4.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00417    0.00244\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     35/100         0G      2.664      5.463      2.545         33        640: 100%|██████████| 1/1 [00:03<00:00,  3.12s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00549    0.00323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     36/100         0G      2.927      6.146        2.7         17        640: 100%|██████████| 1/1 [00:02<00:00,  2.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00549    0.00323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     37/100         0G       2.76       4.98      2.652         40        640: 100%|██████████| 1/1 [00:03<00:00,  3.88s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00547    0.00323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     38/100         0G      2.674      5.154       2.49         35        640: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00547    0.00323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     39/100         0G      2.806      5.385      2.736         30        640: 100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00547    0.00239\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     40/100         0G      2.493       5.57      2.414         19        640: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00547    0.00239\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     41/100         0G      2.197      5.402      2.312         26        640: 100%|██████████| 1/1 [00:03<00:00,  3.04s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00415     0.0018\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     42/100         0G      2.501      5.512      2.684         21        640: 100%|██████████| 1/1 [00:04<00:00,  4.21s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00415     0.0018\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     43/100         0G       2.07      5.352       2.17         18        640: 100%|██████████| 1/1 [00:03<00:00,  3.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00336     0.0017\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     44/100         0G      2.992      5.394      2.856         23        640: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00336     0.0017\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     45/100         0G      2.671      5.116      2.603         36        640: 100%|██████████| 1/1 [00:03<00:00,  3.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00335    0.00143\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     46/100         0G      2.795      5.283      2.717         27        640: 100%|██████████| 1/1 [00:03<00:00,  3.00s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00335    0.00143\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     47/100         0G      2.738      6.351      2.735         12        640: 100%|██████████| 1/1 [00:04<00:00,  4.37s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00282    0.00122\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     48/100         0G      2.732      5.199      2.654         30        640: 100%|██████████| 1/1 [00:02<00:00,  3.00s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00282    0.00122\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     49/100         0G      2.665      5.333      2.763         23        640: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
            "                   all          4         17   0.000139     0.0167     0.0027    0.00109\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     50/100         0G      2.428      5.038      2.648         33        640: 100%|██████████| 1/1 [00:03<00:00,  3.63s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000139     0.0167     0.0027    0.00109\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     51/100         0G      2.581      4.858      2.541         40        640: 100%|██████████| 1/1 [00:03<00:00,  3.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000139     0.0167    0.00402    0.00145\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     52/100         0G      2.743      5.782      2.657         16        640: 100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
            "                   all          4         17   0.000139     0.0167    0.00402    0.00145\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     53/100         0G      2.376      5.082      2.514         29        640: 100%|██████████| 1/1 [00:03<00:00,  3.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
            "                   all          4         17   0.000139     0.0167    0.00402    0.00145\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     54/100         0G      2.767       5.51       2.68         25        640: 100%|██████████| 1/1 [00:03<00:00,  3.04s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
            "                   all          4         17   0.000139     0.0167     0.0027    0.00107\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     55/100         0G      2.584      5.114      2.472         26        640: 100%|██████████| 1/1 [00:04<00:00,  4.19s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000139     0.0167     0.0027    0.00107\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     56/100         0G      2.419      5.172      2.461         31        640: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000139     0.0167     0.0027    0.00107\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     57/100         0G      2.351      4.967      2.461         37        640: 100%|██████████| 1/1 [00:03<00:00,  3.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.00s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00213    0.00102\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     58/100         0G      2.696      5.134      2.705         33        640: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00213    0.00102\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     59/100         0G      2.297      5.186      2.399         31        640: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00213    0.00102\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     60/100         0G      2.354      5.539      2.472         17        640: 100%|██████████| 1/1 [00:04<00:00,  4.40s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
            "                   all          4         17   0.000278     0.0333     0.0017   0.000918\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     61/100         0G      2.655      5.301      2.448         33        640: 100%|██████████| 1/1 [00:03<00:00,  3.02s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000278     0.0333     0.0017   0.000918\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     62/100         0G      2.526      5.621      2.409         21        640: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
            "                   all          4         17   0.000278     0.0333     0.0017   0.000918\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     63/100         0G      2.291      5.342      2.289         24        640: 100%|██████████| 1/1 [00:03<00:00,  3.50s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00164   0.000935\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     64/100         0G      2.821      5.045      2.705         33        640: 100%|██████████| 1/1 [00:02<00:00,  2.99s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00164   0.000935\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     65/100         0G      2.391      5.206      2.425         32        640: 100%|██████████| 1/1 [00:03<00:00,  3.89s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
            "                   all          4         17   0.000278     0.0333    0.00164   0.000935\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     66/100         0G      2.337      4.988      2.417         37        640: 100%|██████████| 1/1 [00:03<00:00,  3.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00126   0.000683\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     67/100         0G      2.286      4.975      2.462         35        640: 100%|██████████| 1/1 [00:02<00:00,  3.00s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00126   0.000683\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     68/100         0G      2.638      5.398      2.687         23        640: 100%|██████████| 1/1 [00:03<00:00,  3.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00126   0.000683\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     69/100         0G      2.763      5.074      2.704         30        640: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00132   0.000686\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     70/100         0G      2.681      5.228      2.709         24        640: 100%|██████████| 1/1 [00:03<00:00,  3.30s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00132   0.000686\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     71/100         0G      2.613      5.044      2.588         35        640: 100%|██████████| 1/1 [00:03<00:00,  3.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00132   0.000686\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     72/100         0G      2.191      4.926       2.28         37        640: 100%|██████████| 1/1 [00:03<00:00,  3.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00146   0.000773\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     73/100         0G      2.478      4.842      2.434         42        640: 100%|██████████| 1/1 [00:04<00:00,  4.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00146   0.000773\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     74/100         0G       2.31      5.245      2.285         26        640: 100%|██████████| 1/1 [00:03<00:00,  3.00s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00146   0.000773\n",
            "Stopping training early as no improvement observed in last 50 epochs. Best results observed at epoch 24, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=50) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "74 epochs completed in 0.109 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.181 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
            "                   all          4         17   0.000417       0.05    0.00839    0.00485\n",
            "                person          4         10     0.0025        0.3     0.0503     0.0291\n",
            "                   dog          4          1          0          0          0          0\n",
            "                 horse          4          2          0          0          0          0\n",
            "              elephant          4          2          0          0          0          0\n",
            "              umbrella          4          1          0          0          0          0\n",
            "          potted plant          4          1          0          0          0          0\n",
            "Speed: 1.7ms preprocess, 235.9ms inference, 0.0ms loss, 28.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "for param in quantized_model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in quantized_model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('quantized_model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqzPFYzrEHKC",
        "outputId": "8ecb5d72-9e03-4937-87a9-7dfc92b00125"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantized_model size: 12.085MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "num_samples = 100\n",
        "start_time = time.time()\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    output = quantized_model(inp)\n",
        "end_time = time.time()\n",
        "\n",
        "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
        "print(f'Avg inference time: {infer_time:.4f} ms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a2334f84947f4cf289c22c8150febf40",
            "0aa24e7ba3df46aa95ccb588a9e234fc",
            "9d4231b0115649a6924e5f681dc3b7c4",
            "0001e2b7debb4bdebd5c09ddd5a4333f",
            "8c8adcd231eb4b1ea82efcd3c42ddae3",
            "925c28d560734ddab1da2378d78d3f4b",
            "02852087b7464b97bc3e46d35f10d948",
            "6b1f90d886ac49e086d68d184e6ccf8b",
            "7f1dc81f3ab44193bfad05c910c73e0c",
            "9579ee28938440fe9635bb2a5bdcc844",
            "82be1a008dba4f05b45545b5df0cee37"
          ]
        },
        "id": "W6Mq58RJEH7d",
        "outputId": "31755015-355a-4453-8ecd-14ff9f8e1802"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2334f84947f4cf289c22c8150febf40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 60.4ms\n",
            "Speed: 0.0ms preprocess, 60.4ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 45.2ms\n",
            "Speed: 0.0ms preprocess, 45.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 45.4ms\n",
            "Speed: 0.0ms preprocess, 45.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 40.8ms\n",
            "Speed: 0.8ms preprocess, 40.8ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 46.6ms\n",
            "Speed: 0.0ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 39.7ms\n",
            "Speed: 0.0ms preprocess, 39.7ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 48.5ms\n",
            "Speed: 0.0ms preprocess, 48.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.9ms\n",
            "Speed: 0.0ms preprocess, 44.9ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 52.3ms\n",
            "Speed: 0.0ms preprocess, 52.3ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 45.6ms\n",
            "Speed: 0.0ms preprocess, 45.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 57.4ms\n",
            "Speed: 0.0ms preprocess, 57.4ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 40.1ms\n",
            "Speed: 0.0ms preprocess, 40.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 61.0ms\n",
            "Speed: 0.0ms preprocess, 61.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.0ms\n",
            "Speed: 0.0ms preprocess, 41.0ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 47.2ms\n",
            "Speed: 0.0ms preprocess, 47.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 39.3ms\n",
            "Speed: 0.0ms preprocess, 39.3ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.3ms\n",
            "Speed: 0.0ms preprocess, 41.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.5ms\n",
            "Speed: 0.0ms preprocess, 44.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.0ms\n",
            "Speed: 0.0ms preprocess, 44.0ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 49.0ms\n",
            "Speed: 0.0ms preprocess, 49.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 46.2ms\n",
            "Speed: 0.0ms preprocess, 46.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 42.3ms\n",
            "Speed: 0.0ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 46.9ms\n",
            "Speed: 0.0ms preprocess, 46.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 49.6ms\n",
            "Speed: 0.0ms preprocess, 49.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 48.4ms\n",
            "Speed: 0.0ms preprocess, 48.4ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.8ms\n",
            "Speed: 0.0ms preprocess, 41.8ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 52.0ms\n",
            "Speed: 0.0ms preprocess, 52.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.0ms\n",
            "Speed: 0.0ms preprocess, 41.0ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 45.4ms\n",
            "Speed: 0.0ms preprocess, 45.4ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 48.1ms\n",
            "Speed: 0.0ms preprocess, 48.1ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.9ms\n",
            "Speed: 0.0ms preprocess, 44.9ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.2ms\n",
            "Speed: 0.0ms preprocess, 41.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 45.8ms\n",
            "Speed: 0.0ms preprocess, 45.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 45.1ms\n",
            "Speed: 0.0ms preprocess, 45.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 43.2ms\n",
            "Speed: 0.0ms preprocess, 43.2ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 39.0ms\n",
            "Speed: 0.0ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 62.3ms\n",
            "Speed: 0.0ms preprocess, 62.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 38.3ms\n",
            "Speed: 0.0ms preprocess, 38.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 43.3ms\n",
            "Speed: 0.0ms preprocess, 43.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 39.9ms\n",
            "Speed: 0.0ms preprocess, 39.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 42.1ms\n",
            "Speed: 0.0ms preprocess, 42.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.2ms\n",
            "Speed: 0.0ms preprocess, 41.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 45.1ms\n",
            "Speed: 0.0ms preprocess, 45.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 54.8ms\n",
            "Speed: 0.0ms preprocess, 54.8ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 49.7ms\n",
            "Speed: 0.0ms preprocess, 49.7ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.8ms\n",
            "Speed: 0.0ms preprocess, 44.8ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.9ms\n",
            "Speed: 0.0ms preprocess, 44.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.2ms\n",
            "Speed: 0.0ms preprocess, 41.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 45.7ms\n",
            "Speed: 0.0ms preprocess, 45.7ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 43.1ms\n",
            "Speed: 0.0ms preprocess, 43.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.3ms\n",
            "Speed: 0.0ms preprocess, 44.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.2ms\n",
            "Speed: 0.0ms preprocess, 41.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.2ms\n",
            "Speed: 0.0ms preprocess, 44.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 40.8ms\n",
            "Speed: 0.0ms preprocess, 40.8ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 45.9ms\n",
            "Speed: 0.0ms preprocess, 45.9ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 42.3ms\n",
            "Speed: 0.0ms preprocess, 42.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 47.1ms\n",
            "Speed: 0.0ms preprocess, 47.1ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.2ms\n",
            "Speed: 0.0ms preprocess, 41.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 47.5ms\n",
            "Speed: 0.0ms preprocess, 47.5ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 43.8ms\n",
            "Speed: 0.0ms preprocess, 43.8ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.1ms\n",
            "Speed: 0.0ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 46.1ms\n",
            "Speed: 0.0ms preprocess, 46.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 48.1ms\n",
            "Speed: 0.0ms preprocess, 48.1ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.0ms\n",
            "Speed: 0.0ms preprocess, 44.0ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.1ms\n",
            "Speed: 0.0ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 42.2ms\n",
            "Speed: 0.0ms preprocess, 42.2ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 48.6ms\n",
            "Speed: 0.0ms preprocess, 48.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 40.6ms\n",
            "Speed: 0.0ms preprocess, 40.6ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 45.3ms\n",
            "Speed: 0.0ms preprocess, 45.3ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.4ms\n",
            "Speed: 0.0ms preprocess, 41.4ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 46.6ms\n",
            "Speed: 0.0ms preprocess, 46.6ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 38.5ms\n",
            "Speed: 0.0ms preprocess, 38.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.0ms\n",
            "Speed: 0.0ms preprocess, 44.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 41.0ms\n",
            "Speed: 0.0ms preprocess, 41.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 44.3ms\n",
            "Speed: 0.0ms preprocess, 44.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 65.7ms\n",
            "Speed: 0.0ms preprocess, 65.7ms inference, 1.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 76.5ms\n",
            "Speed: 0.0ms preprocess, 76.5ms inference, 2.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 66.7ms\n",
            "Speed: 0.0ms preprocess, 66.7ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 77.2ms\n",
            "Speed: 0.0ms preprocess, 77.2ms inference, 3.5ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 65.8ms\n",
            "Speed: 0.0ms preprocess, 65.8ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 60.1ms\n",
            "Speed: 0.0ms preprocess, 60.1ms inference, 1.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 65.4ms\n",
            "Speed: 0.0ms preprocess, 65.4ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 59.0ms\n",
            "Speed: 0.0ms preprocess, 59.0ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 67.8ms\n",
            "Speed: 0.0ms preprocess, 67.8ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 62.8ms\n",
            "Speed: 0.0ms preprocess, 62.8ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 59.7ms\n",
            "Speed: 0.0ms preprocess, 59.7ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 59.4ms\n",
            "Speed: 0.0ms preprocess, 59.4ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 60.8ms\n",
            "Speed: 0.0ms preprocess, 60.8ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 62.7ms\n",
            "Speed: 0.0ms preprocess, 62.7ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 59.7ms\n",
            "Speed: 0.0ms preprocess, 59.7ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 59.5ms\n",
            "Speed: 0.0ms preprocess, 59.5ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 67.8ms\n",
            "Speed: 0.0ms preprocess, 67.8ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 68.1ms\n",
            "Speed: 0.0ms preprocess, 68.1ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 60.6ms\n",
            "Speed: 0.0ms preprocess, 60.6ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 62.6ms\n",
            "Speed: 0.0ms preprocess, 62.6ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 59.3ms\n",
            "Speed: 0.0ms preprocess, 59.3ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 64.4ms\n",
            "Speed: 0.0ms preprocess, 64.4ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 60.6ms\n",
            "Speed: 0.0ms preprocess, 60.6ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 59.3ms\n",
            "Speed: 0.0ms preprocess, 59.3ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.253537178039551. Dividing input by 255.\n",
            "0: 224x224 (no detections), 60.3ms\n",
            "Speed: 0.0ms preprocess, 60.3ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg inference time: 63.7313 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import prune"
      ],
      "metadata": {
        "id": "ifmcxX8REyLK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, m in model.named_modules():\n",
        "  params = list(m.named_parameters())\n",
        "  if len(params) and params[0][0] == 'weight':\n",
        "    prune.l1_unstructured(m, name=params[0][0], amount=0.3)"
      ],
      "metadata": {
        "id": "Iaa-kQhhGBsO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf0UxbbqKkAO",
        "outputId": "f725b17a-63ff-40e2-a928-071dd5c54da6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 24.025MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "num_samples = 100\n",
        "start_time = time.time()\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    output = model(inp / 255)\n",
        "end_time = time.time()\n",
        "\n",
        "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
        "print(f'Avg inference time: {infer_time:.4f} ms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c47c70320a65487db3819c445336b1d0",
            "9ac30dd0b0f3436ca22111620247d2a3",
            "2c8e77c01eba4fc8a52e4b65a5dc868f",
            "4e2668d003d14df0a7907f616785f66b",
            "a2e048953ddb459daeeab913c86c38d8",
            "e3bc08d2f1cf4dad9323b67fb461e6ff",
            "b94551a2b29d471e9e3b92dfd221bfbd",
            "d56695cd421b4c218eb151a8be19fc80",
            "0f60fd9ae6134e8c856876d1bce53fcd",
            "1482ba59c7024fecb8b5b6353bbe210c",
            "e7912bd1137f409fba47ecec7ca027e6"
          ]
        },
        "id": "i9r6ZqnjK23D",
        "outputId": "3954a77f-6d64-41a7-fc49-4d415b02d363"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c47c70320a65487db3819c445336b1d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 224x224 (no detections), 83.7ms\n",
            "Speed: 0.1ms preprocess, 83.7ms inference, 3.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 72.9ms\n",
            "Speed: 0.0ms preprocess, 72.9ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 51.5ms\n",
            "Speed: 0.0ms preprocess, 51.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 48.7ms\n",
            "Speed: 0.0ms preprocess, 48.7ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 50.5ms\n",
            "Speed: 0.0ms preprocess, 50.5ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 53.7ms\n",
            "Speed: 0.0ms preprocess, 53.7ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.6ms\n",
            "Speed: 0.0ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.6ms\n",
            "Speed: 0.0ms preprocess, 49.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.8ms\n",
            "Speed: 0.0ms preprocess, 45.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 48.7ms\n",
            "Speed: 0.0ms preprocess, 48.7ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.1ms\n",
            "Speed: 0.0ms preprocess, 46.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 56.6ms\n",
            "Speed: 0.0ms preprocess, 56.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.0ms\n",
            "Speed: 0.0ms preprocess, 45.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 55.6ms\n",
            "Speed: 0.0ms preprocess, 55.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 48.7ms\n",
            "Speed: 0.0ms preprocess, 48.7ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.8ms\n",
            "Speed: 0.0ms preprocess, 49.8ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.8ms\n",
            "Speed: 0.0ms preprocess, 45.8ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 51.7ms\n",
            "Speed: 0.0ms preprocess, 51.7ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 52.5ms\n",
            "Speed: 0.0ms preprocess, 52.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.1ms\n",
            "Speed: 0.2ms preprocess, 49.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 44.1ms\n",
            "Speed: 0.0ms preprocess, 44.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 51.6ms\n",
            "Speed: 0.0ms preprocess, 51.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 59.5ms\n",
            "Speed: 0.0ms preprocess, 59.5ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.0ms\n",
            "Speed: 0.0ms preprocess, 49.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.0ms\n",
            "Speed: 0.0ms preprocess, 46.0ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 51.0ms\n",
            "Speed: 0.0ms preprocess, 51.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.4ms\n",
            "Speed: 0.0ms preprocess, 46.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.8ms\n",
            "Speed: 0.0ms preprocess, 49.8ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 51.2ms\n",
            "Speed: 0.0ms preprocess, 51.2ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 51.2ms\n",
            "Speed: 0.0ms preprocess, 51.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.3ms\n",
            "Speed: 0.0ms preprocess, 45.3ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 55.4ms\n",
            "Speed: 0.0ms preprocess, 55.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 47.0ms\n",
            "Speed: 0.0ms preprocess, 47.0ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 51.0ms\n",
            "Speed: 0.0ms preprocess, 51.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.6ms\n",
            "Speed: 0.0ms preprocess, 46.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 51.2ms\n",
            "Speed: 0.0ms preprocess, 51.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 61.3ms\n",
            "Speed: 0.0ms preprocess, 61.3ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 75.9ms\n",
            "Speed: 0.0ms preprocess, 75.9ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 79.6ms\n",
            "Speed: 0.0ms preprocess, 79.6ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 73.7ms\n",
            "Speed: 0.0ms preprocess, 73.7ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 69.6ms\n",
            "Speed: 0.0ms preprocess, 69.6ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 71.8ms\n",
            "Speed: 0.0ms preprocess, 71.8ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 77.9ms\n",
            "Speed: 0.0ms preprocess, 77.9ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 83.2ms\n",
            "Speed: 0.0ms preprocess, 83.2ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 76.7ms\n",
            "Speed: 0.0ms preprocess, 76.7ms inference, 1.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 68.0ms\n",
            "Speed: 0.0ms preprocess, 68.0ms inference, 1.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 67.4ms\n",
            "Speed: 0.0ms preprocess, 67.4ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 70.4ms\n",
            "Speed: 0.0ms preprocess, 70.4ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 67.0ms\n",
            "Speed: 0.0ms preprocess, 67.0ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 67.8ms\n",
            "Speed: 0.0ms preprocess, 67.8ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 67.7ms\n",
            "Speed: 0.0ms preprocess, 67.7ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 79.1ms\n",
            "Speed: 0.0ms preprocess, 79.1ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 72.7ms\n",
            "Speed: 0.0ms preprocess, 72.7ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 71.8ms\n",
            "Speed: 0.0ms preprocess, 71.8ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 71.7ms\n",
            "Speed: 0.0ms preprocess, 71.7ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 73.9ms\n",
            "Speed: 0.0ms preprocess, 73.9ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 73.1ms\n",
            "Speed: 0.2ms preprocess, 73.1ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 71.1ms\n",
            "Speed: 0.0ms preprocess, 71.1ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 74.8ms\n",
            "Speed: 0.0ms preprocess, 74.8ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 68.5ms\n",
            "Speed: 0.0ms preprocess, 68.5ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 67.5ms\n",
            "Speed: 0.0ms preprocess, 67.5ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 73.3ms\n",
            "Speed: 0.0ms preprocess, 73.3ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 68.6ms\n",
            "Speed: 0.0ms preprocess, 68.6ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 79.7ms\n",
            "Speed: 0.0ms preprocess, 79.7ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 71.4ms\n",
            "Speed: 0.0ms preprocess, 71.4ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 80.3ms\n",
            "Speed: 0.0ms preprocess, 80.3ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 70.3ms\n",
            "Speed: 0.0ms preprocess, 70.3ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 71.2ms\n",
            "Speed: 0.0ms preprocess, 71.2ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 74.4ms\n",
            "Speed: 0.0ms preprocess, 74.4ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 75.6ms\n",
            "Speed: 0.1ms preprocess, 75.6ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 73.1ms\n",
            "Speed: 0.0ms preprocess, 73.1ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 80.4ms\n",
            "Speed: 0.0ms preprocess, 80.4ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 68.5ms\n",
            "Speed: 0.0ms preprocess, 68.5ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 74.6ms\n",
            "Speed: 0.0ms preprocess, 74.6ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 69.8ms\n",
            "Speed: 0.0ms preprocess, 69.8ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 74.2ms\n",
            "Speed: 0.0ms preprocess, 74.2ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 73.1ms\n",
            "Speed: 0.0ms preprocess, 73.1ms inference, 1.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 73.1ms\n",
            "Speed: 0.0ms preprocess, 73.1ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 71.2ms\n",
            "Speed: 0.0ms preprocess, 71.2ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 78.1ms\n",
            "Speed: 0.0ms preprocess, 78.1ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 81.3ms\n",
            "Speed: 0.0ms preprocess, 81.3ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.1ms\n",
            "Speed: 0.0ms preprocess, 49.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.4ms\n",
            "Speed: 0.0ms preprocess, 45.4ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 52.5ms\n",
            "Speed: 0.0ms preprocess, 52.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.2ms\n",
            "Speed: 0.0ms preprocess, 46.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 49.1ms\n",
            "Speed: 0.0ms preprocess, 49.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.5ms\n",
            "Speed: 0.0ms preprocess, 46.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 53.2ms\n",
            "Speed: 0.0ms preprocess, 53.2ms inference, 1.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.2ms\n",
            "Speed: 0.0ms preprocess, 46.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 48.6ms\n",
            "Speed: 0.0ms preprocess, 48.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.2ms\n",
            "Speed: 0.0ms preprocess, 45.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 54.4ms\n",
            "Speed: 0.0ms preprocess, 54.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 47.7ms\n",
            "Speed: 0.0ms preprocess, 47.7ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 53.7ms\n",
            "Speed: 0.0ms preprocess, 53.7ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 45.1ms\n",
            "Speed: 0.0ms preprocess, 45.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 59.3ms\n",
            "Speed: 0.0ms preprocess, 59.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 52.1ms\n",
            "Speed: 0.0ms preprocess, 52.1ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 52.0ms\n",
            "Speed: 0.0ms preprocess, 52.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 46.4ms\n",
            "Speed: 0.0ms preprocess, 46.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 55.4ms\n",
            "Speed: 0.0ms preprocess, 55.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg inference time: 72.6070 sec\n"
          ]
        }
      ]
    }
  ]
}