{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe0e8GY6eCFY",
        "outputId": "963d847d-e099-4e8e-962d-43efe16f4fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/618.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/618.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m460.8/618.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.9/618.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdftgk5teJaR"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeciXJ7luC-0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import models\n",
        "from torch.nn.utils import prune\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8pPy3xW-_ax"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmTPgMIxeOK3",
        "outputId": "06f7fb10-8414-4f7a-e6ab-949df5369536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt to 'yolov8l.pt'...\n",
            "100%|██████████| 83.7M/83.7M [00:01<00:00, 82.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = YOLO('yolov8l.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGKXt-yEBGLt",
        "outputId": "80bfc5ab-4352-4c9d-da06-172b0495a7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 166.848MB\n"
          ]
        }
      ],
      "source": [
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "O8qjf7N8ybaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!yolo task=detect mode=val model=yolov8l.pt name=yolov8l_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkS2jkSfaWLl",
        "outputId": "c8d722d4-4e55-4301-cd77-e0a87c55c13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ 'data' is missing. Using default 'data=coco8.yaml'.\n",
            "Ultralytics YOLOv8.0.190 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8l summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n",
            "\n",
            "Dataset 'coco8.yaml' images not found ⚠️, missing path '/content/datasets/coco8/images/val'\n",
            "Downloading https://ultralytics.com/assets/coco8.zip to '/content/datasets/coco8.zip'...\n",
            "100% 433k/433k [00:00<00:00, 14.0MB/s]\n",
            "Unzipping /content/datasets/coco8.zip to /content/datasets/coco8...: 100% 25/25 [00:00<00:00, 2758.32file/s]\n",
            "Dataset download success ✅ (0.7s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100% 4/4 [00:00<00:00, 233.66it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8/labels/val.cache\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:15<00:00, 15.39s/it]\n",
            "                   all          4         17      0.914      0.839      0.976       0.76\n",
            "                person          4         10          1       0.42       0.88      0.574\n",
            "                   dog          4          1      0.869          1      0.995      0.895\n",
            "                 horse          4          2      0.902          1      0.995      0.681\n",
            "              elephant          4          2          1      0.612      0.995      0.619\n",
            "              umbrella          4          1      0.822          1      0.995      0.995\n",
            "          potted plant          4          1      0.891          1      0.995      0.796\n",
            "Speed: 20.9ms preprocess, 3808.6ms inference, 0.0ms loss, 3.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/yolov8l_eval\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "48d5c76fe9b04937bf110f0649a53651",
            "978e791731214ca68096d0666658312e",
            "c7001d56b4de4dfc88a22413fe531d7a",
            "9b91b3aa90094e179703b93095af60dd",
            "85a8b3a8910d4cc3bf8456536a68d5f1",
            "b04c77be194449f1b44a57565be3b706",
            "d2afef5ef82340ecb172a158327cb178",
            "773899e4716842539f3f4fa83b4d4030",
            "0ac34fc27d3c4f06ab78be2c34ab7612",
            "66713c26ca174271a4f0dabeea233768",
            "dacf824c34a349c28ebf563c578f0f63"
          ]
        },
        "id": "xPh-Kyu9_ssN",
        "outputId": "f4ab6df5-cd8d-4a26-eb46-8654ebd56cfe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48d5c76fe9b04937bf110f0649a53651"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 224x224 (no detections), 704.2ms\n",
            "Speed: 0.0ms preprocess, 704.2ms inference, 13.4ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 511.4ms\n",
            "Speed: 0.0ms preprocess, 511.4ms inference, 1.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 621.6ms\n",
            "Speed: 0.0ms preprocess, 621.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 454.2ms\n",
            "Speed: 0.0ms preprocess, 454.2ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 446.7ms\n",
            "Speed: 0.0ms preprocess, 446.7ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 411.2ms\n",
            "Speed: 0.0ms preprocess, 411.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 444.7ms\n",
            "Speed: 0.0ms preprocess, 444.7ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 417.1ms\n",
            "Speed: 0.1ms preprocess, 417.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 423.4ms\n",
            "Speed: 0.0ms preprocess, 423.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 460.2ms\n",
            "Speed: 0.0ms preprocess, 460.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 285.5ms\n",
            "Speed: 0.0ms preprocess, 285.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 281.4ms\n",
            "Speed: 0.0ms preprocess, 281.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 302.7ms\n",
            "Speed: 0.0ms preprocess, 302.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 288.3ms\n",
            "Speed: 0.0ms preprocess, 288.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 279.7ms\n",
            "Speed: 0.0ms preprocess, 279.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 287.9ms\n",
            "Speed: 0.0ms preprocess, 287.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 292.5ms\n",
            "Speed: 0.0ms preprocess, 292.5ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 283.0ms\n",
            "Speed: 0.0ms preprocess, 283.0ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 279.6ms\n",
            "Speed: 0.0ms preprocess, 279.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 287.0ms\n",
            "Speed: 0.0ms preprocess, 287.0ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 281.3ms\n",
            "Speed: 0.0ms preprocess, 281.3ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 286.1ms\n",
            "Speed: 0.0ms preprocess, 286.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 286.2ms\n",
            "Speed: 0.0ms preprocess, 286.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 366.1ms\n",
            "Speed: 0.0ms preprocess, 366.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 393.0ms\n",
            "Speed: 0.0ms preprocess, 393.0ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 394.3ms\n",
            "Speed: 0.0ms preprocess, 394.3ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 391.0ms\n",
            "Speed: 0.0ms preprocess, 391.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 378.4ms\n",
            "Speed: 0.0ms preprocess, 378.4ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 364.4ms\n",
            "Speed: 0.0ms preprocess, 364.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 291.7ms\n",
            "Speed: 0.0ms preprocess, 291.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 289.2ms\n",
            "Speed: 0.0ms preprocess, 289.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 300.2ms\n",
            "Speed: 0.0ms preprocess, 300.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 276.3ms\n",
            "Speed: 0.0ms preprocess, 276.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 283.2ms\n",
            "Speed: 0.0ms preprocess, 283.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 288.7ms\n",
            "Speed: 0.0ms preprocess, 288.7ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 285.0ms\n",
            "Speed: 0.0ms preprocess, 285.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 284.9ms\n",
            "Speed: 0.0ms preprocess, 284.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 278.7ms\n",
            "Speed: 0.0ms preprocess, 278.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 291.9ms\n",
            "Speed: 0.0ms preprocess, 291.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 294.6ms\n",
            "Speed: 0.0ms preprocess, 294.6ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 289.6ms\n",
            "Speed: 0.0ms preprocess, 289.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 295.4ms\n",
            "Speed: 0.0ms preprocess, 295.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 285.8ms\n",
            "Speed: 0.0ms preprocess, 285.8ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 286.1ms\n",
            "Speed: 0.0ms preprocess, 286.1ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 287.9ms\n",
            "Speed: 0.0ms preprocess, 287.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 289.7ms\n",
            "Speed: 0.0ms preprocess, 289.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 293.1ms\n",
            "Speed: 0.0ms preprocess, 293.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 287.1ms\n",
            "Speed: 0.0ms preprocess, 287.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 304.2ms\n",
            "Speed: 0.0ms preprocess, 304.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 290.9ms\n",
            "Speed: 0.0ms preprocess, 290.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 285.6ms\n",
            "Speed: 0.0ms preprocess, 285.6ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 288.2ms\n",
            "Speed: 0.0ms preprocess, 288.2ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 297.5ms\n",
            "Speed: 0.0ms preprocess, 297.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 284.3ms\n",
            "Speed: 0.0ms preprocess, 284.3ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 284.0ms\n",
            "Speed: 0.0ms preprocess, 284.0ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 288.5ms\n",
            "Speed: 0.0ms preprocess, 288.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 291.0ms\n",
            "Speed: 0.0ms preprocess, 291.0ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 278.8ms\n",
            "Speed: 0.0ms preprocess, 278.8ms inference, 0.6ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 291.3ms\n",
            "Speed: 0.0ms preprocess, 291.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 287.8ms\n",
            "Speed: 0.0ms preprocess, 287.8ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 303.5ms\n",
            "Speed: 0.0ms preprocess, 303.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 297.2ms\n",
            "Speed: 0.0ms preprocess, 297.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 401.0ms\n",
            "Speed: 0.0ms preprocess, 401.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 396.4ms\n",
            "Speed: 0.0ms preprocess, 396.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 404.3ms\n",
            "Speed: 0.0ms preprocess, 404.3ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 408.1ms\n",
            "Speed: 0.0ms preprocess, 408.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 412.9ms\n",
            "Speed: 0.0ms preprocess, 412.9ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 351.1ms\n",
            "Speed: 0.0ms preprocess, 351.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 299.4ms\n",
            "Speed: 0.0ms preprocess, 299.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 304.5ms\n",
            "Speed: 0.0ms preprocess, 304.5ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 321.9ms\n",
            "Speed: 0.0ms preprocess, 321.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 307.1ms\n",
            "Speed: 0.0ms preprocess, 307.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 316.1ms\n",
            "Speed: 0.0ms preprocess, 316.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 315.2ms\n",
            "Speed: 0.0ms preprocess, 315.2ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 335.9ms\n",
            "Speed: 0.0ms preprocess, 335.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 331.6ms\n",
            "Speed: 0.0ms preprocess, 331.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 347.4ms\n",
            "Speed: 0.0ms preprocess, 347.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 328.1ms\n",
            "Speed: 0.0ms preprocess, 328.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 324.2ms\n",
            "Speed: 0.0ms preprocess, 324.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 319.3ms\n",
            "Speed: 0.0ms preprocess, 319.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 315.1ms\n",
            "Speed: 0.0ms preprocess, 315.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 329.7ms\n",
            "Speed: 0.0ms preprocess, 329.7ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 336.9ms\n",
            "Speed: 0.0ms preprocess, 336.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 316.1ms\n",
            "Speed: 0.0ms preprocess, 316.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 313.2ms\n",
            "Speed: 0.0ms preprocess, 313.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 314.9ms\n",
            "Speed: 0.0ms preprocess, 314.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 322.9ms\n",
            "Speed: 0.0ms preprocess, 322.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 330.2ms\n",
            "Speed: 0.0ms preprocess, 330.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 326.9ms\n",
            "Speed: 0.0ms preprocess, 326.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 315.2ms\n",
            "Speed: 0.0ms preprocess, 315.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 301.7ms\n",
            "Speed: 0.0ms preprocess, 301.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 312.7ms\n",
            "Speed: 0.0ms preprocess, 312.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 329.9ms\n",
            "Speed: 0.0ms preprocess, 329.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 303.3ms\n",
            "Speed: 0.0ms preprocess, 303.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 307.1ms\n",
            "Speed: 0.0ms preprocess, 307.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 317.4ms\n",
            "Speed: 0.0ms preprocess, 317.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 310.8ms\n",
            "Speed: 0.0ms preprocess, 310.8ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 334.2ms\n",
            "Speed: 0.0ms preprocess, 334.2ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 431.0ms\n",
            "Speed: 0.0ms preprocess, 431.0ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 439.0ms\n",
            "Speed: 0.0ms preprocess, 439.0ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg inference time: 378.9708 ms\n"
          ]
        }
      ],
      "source": [
        "inp = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "num_samples = 100\n",
        "start_time = time.time()\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    output = model(inp / 255)\n",
        "end_time = time.time()\n",
        "\n",
        "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
        "print(f'Avg inference time: {infer_time:.4f} ms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efmh803MBgxe",
        "outputId": "2f77b8ec-583d-42f0-95a6-e64a57dcc533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg inference time: 378.9708 ms\n"
          ]
        }
      ],
      "source": [
        "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
        "print(f'Avg inference time: {infer_time:.4f} ms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57AnOq0ft-Bs",
        "outputId": "ef7ab024-e27c-4845-b8f0-97f5affd4182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO(\n",
            "  (model): DetectionModel(\n",
            "    (model): Sequential(\n",
            "      (0): Conv(\n",
            "        (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (1): Conv(\n",
            "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (2): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-2): 3 x Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): Conv(\n",
            "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (4): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-5): 6 x Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (5): Conv(\n",
            "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (6): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-5): 6 x Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (7): Conv(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (8): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-2): 3 x Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (9): SPPF(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
            "      )\n",
            "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
            "      (11): Concat()\n",
            "      (12): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-2): 3 x Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
            "      (14): Concat()\n",
            "      (15): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(640, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-2): 3 x Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (16): Conv(\n",
            "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (17): Concat()\n",
            "      (18): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-2): 3 x Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (19): Conv(\n",
            "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (20): Concat()\n",
            "      (21): C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-2): 3 x Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (22): Detect(\n",
            "        (cv2): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (1-2): 2 x Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (cv3): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (1-2): 2 x Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (dfl): DFL(\n",
            "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Квантизация"
      ],
      "metadata": {
        "id": "jiFR3uIdrWen"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgffMzZpCCLn",
        "outputId": "8de56990-037a-4543-d5b4-650eb99f9188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.194 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (AMD EPYC 7B12)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=coco8.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "\n",
            "Dataset 'coco8.yaml' images not found ⚠️, missing path '/content/datasets/coco8/images/val'\n",
            "Downloading https://ultralytics.com/assets/coco8.zip to '/content/datasets/coco8.zip'...\n",
            "100%|██████████| 433k/433k [00:00<00:00, 9.96MB/s]\n",
            "Unzipping /content/datasets/coco8.zip to /content/datasets/coco8...: 100%|██████████| 25/25 [00:00<00:00, 4324.74file/s]\n",
            "Dataset download success ✅ (1.1s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100%|██████████| 755k/755k [00:00<00:00, 14.3MB/s]\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
            "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
            "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
            " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
            " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 22        [15, 18, 21]  1   5644480  ultralytics.nn.modules.head.Detect           [80, [256, 512, 512]]         \n",
            "Model summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs\n",
            "\n",
            "Transferred 110/595 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8/labels/train... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 307.26it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco8/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<00:00, 1025.94it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8/labels/val.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/100         0G      2.603      4.775      2.618         37        640: 100%|██████████| 1/1 [00:30<00:00, 30.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.70s/it]\n",
            "                   all          4         17    0.00165     0.0167    0.00392   0.000392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      2/100         0G      2.798      5.255      2.995         39        640: 100%|██████████| 1/1 [00:32<00:00, 32.83s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.10s/it]\n",
            "                   all          4         17    0.00165     0.0167    0.00392   0.000392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      3/100         0G      2.519      5.604      3.034         18        640: 100%|██████████| 1/1 [00:31<00:00, 31.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.71s/it]\n",
            "                   all          4         17    0.00165     0.0167    0.00392   0.000392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      4/100         0G      2.851      5.042      2.913         43        640: 100%|██████████| 1/1 [00:30<00:00, 30.26s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.72s/it]\n",
            "                   all          4         17    0.00165     0.0167    0.00392   0.000392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      5/100         0G      2.759      5.525       2.93         18        640: 100%|██████████| 1/1 [00:29<00:00, 29.37s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.37s/it]\n",
            "                   all          4         17    0.00165     0.0167    0.00392   0.000392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      6/100         0G      2.598      4.955       2.72         41        640: 100%|██████████| 1/1 [00:31<00:00, 31.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.77s/it]\n",
            "                   all          4         17    0.00165     0.0167    0.00392   0.000392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      7/100         0G      2.985        5.4      3.078         19        640: 100%|██████████| 1/1 [00:31<00:00, 31.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.08s/it]\n",
            "                   all          4         17    0.00165     0.0167    0.00392   0.000392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      8/100         0G      2.563      5.254      2.755         23        640: 100%|██████████| 1/1 [00:37<00:00, 37.99s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.57s/it]\n",
            "                   all          4         17    0.00165     0.0167    0.00392   0.000392\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      9/100         0G      2.544      5.018      2.729         36        640: 100%|██████████| 1/1 [00:30<00:00, 30.56s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.72s/it]\n",
            "                   all          4         17    0.00185     0.0167    0.00113   0.000113\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     10/100         0G       2.81      5.369      3.014         16        640: 100%|██████████| 1/1 [00:31<00:00, 31.63s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.89s/it]\n",
            "                   all          4         17    0.00262     0.0333    0.00356   0.000356\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     11/100         0G      2.854      5.498      2.898         13        640: 100%|██████████| 1/1 [00:31<00:00, 31.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.90s/it]\n",
            "                   all          4         17    0.00134     0.0167    0.00118   0.000118\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     12/100         0G      2.682      5.022      2.766         36        640: 100%|██████████| 1/1 [00:30<00:00, 30.28s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.25s/it]\n",
            "                   all          4         17    0.00111     0.0167   0.000835   8.35e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     13/100         0G       2.36      4.858       2.69         31        640: 100%|██████████| 1/1 [00:29<00:00, 29.22s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.60s/it]\n",
            "                   all          4         17    0.00176     0.0333    0.00545    0.00109\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     14/100         0G      2.285      4.659      2.396         33        640: 100%|██████████| 1/1 [00:29<00:00, 29.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.51s/it]\n",
            "                   all          4         17   0.000656     0.0167   0.000379   0.000114\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     15/100         0G      2.464      5.709      2.699         11        640: 100%|██████████| 1/1 [00:29<00:00, 29.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.66s/it]\n",
            "                   all          4         17    0.00178       0.05    0.00307   0.000346\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     16/100         0G      2.425       4.74      2.656         31        640: 100%|██████████| 1/1 [00:29<00:00, 29.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n",
            "                   all          4         17    0.00104     0.0333    0.00316   0.000374\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     17/100         0G      2.452      4.579      2.602         38        640: 100%|██████████| 1/1 [00:30<00:00, 30.52s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.66s/it]\n",
            "                   all          4         17    0.00079     0.0333    0.00278   0.000347\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     18/100         0G      2.694      4.847      2.892         19        640: 100%|██████████| 1/1 [00:28<00:00, 28.41s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.55s/it]\n",
            "                   all          4         17    0.00079     0.0333    0.00278   0.000347\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     19/100         0G      2.477      4.782      2.682         24        640: 100%|██████████| 1/1 [00:29<00:00, 29.44s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.26s/it]\n",
            "                   all          4         17    0.00079     0.0333    0.00278   0.000347\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     20/100         0G      2.116      4.679      2.512         28        640: 100%|██████████| 1/1 [00:29<00:00, 29.21s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.72s/it]\n",
            "                   all          4         17    0.00079     0.0333    0.00278   0.000347\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     21/100         0G       2.12      4.449      2.315         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.51s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:12<00:00, 12.04s/it]\n",
            "                   all          4         17    0.00079     0.0333    0.00278   0.000347\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     22/100         0G      2.314      4.748      2.476         32        640: 100%|██████████| 1/1 [00:28<00:00, 28.65s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.57s/it]\n",
            "                   all          4         17    0.00079     0.0333    0.00278   0.000347\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     23/100         0G      2.351      4.396      2.394         41        640: 100%|██████████| 1/1 [00:29<00:00, 29.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.66s/it]\n",
            "                   all          4         17    0.00079     0.0333    0.00278   0.000347\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     24/100         0G      2.623      4.563      2.654         29        640: 100%|██████████| 1/1 [00:29<00:00, 29.50s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.63s/it]\n",
            "                   all          4         17    0.00079     0.0333    0.00278   0.000347\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     25/100         0G       2.41      4.615      2.563         29        640: 100%|██████████| 1/1 [00:28<00:00, 28.90s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.32s/it]\n",
            "                   all          4         17   0.000145     0.0167    0.00233   0.000233\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     26/100         0G      2.021      4.353      2.226         32        640: 100%|██████████| 1/1 [00:28<00:00, 28.79s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.60s/it]\n",
            "                   all          4         17   0.000145     0.0167    0.00233   0.000233\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     27/100         0G      2.331      4.232      2.442         32        640: 100%|██████████| 1/1 [00:28<00:00, 28.89s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.50s/it]\n",
            "                   all          4         17   0.000436       0.05    0.00344   0.000418\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     28/100         0G      1.711      4.213      2.221         16        640: 100%|██████████| 1/1 [00:28<00:00, 28.75s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.77s/it]\n",
            "                   all          4         17   0.000436       0.05    0.00344   0.000418\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     29/100         0G      2.099       4.64      2.534         15        640: 100%|██████████| 1/1 [00:29<00:00, 29.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.18s/it]\n",
            "                   all          4         17      0.167      0.217      0.169     0.0999\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     30/100         0G      2.202      4.161      2.209         23        640: 100%|██████████| 1/1 [00:28<00:00, 28.39s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.45s/it]\n",
            "                   all          4         17      0.167      0.217      0.169     0.0999\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     31/100         0G       2.09      4.174      2.181         31        640: 100%|██████████| 1/1 [00:29<00:00, 29.12s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.63s/it]\n",
            "                   all          4         17      0.167      0.217      0.168     0.0998\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     32/100         0G      1.641      3.951      2.102         13        640: 100%|██████████| 1/1 [00:28<00:00, 28.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n",
            "                   all          4         17      0.167      0.217      0.168     0.0998\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     33/100         0G      1.843      3.931      2.067         43        640: 100%|██████████| 1/1 [00:29<00:00, 29.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.53s/it]\n",
            "                   all          4         17   0.000613     0.0667    0.00303   0.000563\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     34/100         0G      2.072      4.073      2.273         36        640: 100%|██████████| 1/1 [00:28<00:00, 28.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.57s/it]\n",
            "                   all          4         17   0.000613     0.0667    0.00303   0.000563\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     35/100         0G      2.212      4.217      2.377         33        640: 100%|██████████| 1/1 [00:29<00:00, 29.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.59s/it]\n",
            "                   all          4         17   0.000767     0.0833    0.00419     0.0007\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     36/100         0G      2.413      4.721      2.493         17        640: 100%|██████████| 1/1 [00:31<00:00, 31.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n",
            "                   all          4         17   0.000767     0.0833    0.00419     0.0007\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     37/100         0G      2.097      3.903      2.205         40        640: 100%|██████████| 1/1 [00:28<00:00, 28.83s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.76s/it]\n",
            "                   all          4         17      0.167      0.233       0.17      0.117\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     38/100         0G      2.116      3.993      2.262         35        640: 100%|██████████| 1/1 [00:29<00:00, 29.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.76s/it]\n",
            "                   all          4         17      0.167      0.233       0.17      0.117\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     39/100         0G      2.095      3.885      2.266         30        640: 100%|██████████| 1/1 [00:29<00:00, 29.59s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.27s/it]\n",
            "                   all          4         17   0.000885        0.1    0.00538    0.00105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     40/100         0G      1.866      3.679      2.056         19        640: 100%|██████████| 1/1 [00:28<00:00, 28.94s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.84s/it]\n",
            "                   all          4         17   0.000885        0.1    0.00538    0.00105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     41/100         0G      2.045      4.068      2.124         26        640: 100%|██████████| 1/1 [00:31<00:00, 31.78s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.79s/it]\n",
            "                   all          4         17   0.000758     0.0833    0.00502    0.00123\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     42/100         0G      1.935      3.986      2.331         21        640: 100%|██████████| 1/1 [00:29<00:00, 29.81s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.92s/it]\n",
            "                   all          4         17   0.000758     0.0833    0.00502    0.00123\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     43/100         0G       1.85      4.048      2.176         18        640: 100%|██████████| 1/1 [00:30<00:00, 30.51s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.91s/it]\n",
            "                   all          4         17    0.00076     0.0833     0.0034    0.00105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     44/100         0G      2.073      3.821      2.245         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.61s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.97s/it]\n",
            "                   all          4         17    0.00076     0.0833     0.0034    0.00105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     45/100         0G      2.195      3.946      2.296         36        640: 100%|██████████| 1/1 [00:31<00:00, 31.83s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.62s/it]\n",
            "                   all          4         17   0.000758     0.0833    0.00343    0.00103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     46/100         0G      2.124      3.719      2.276         27        640: 100%|██████████| 1/1 [00:31<00:00, 31.20s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.87s/it]\n",
            "                   all          4         17   0.000758     0.0833    0.00343    0.00103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     47/100         0G      1.853      3.993      2.269         12        640: 100%|██████████| 1/1 [00:31<00:00, 31.78s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.47s/it]\n",
            "                   all          4         17   0.000768     0.0833    0.00352    0.00105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     48/100         0G      1.985      3.737      2.258         30        640: 100%|██████████| 1/1 [00:29<00:00, 29.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:11<00:00, 11.71s/it]\n",
            "                   all          4         17   0.000768     0.0833    0.00352    0.00105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     49/100         0G      1.922       3.88      2.358         23        640: 100%|██████████| 1/1 [00:30<00:00, 30.17s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.67s/it]\n",
            "                   all          4         17   0.000929        0.1     0.0028   0.000686\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     50/100         0G       1.66      3.559      2.108         33        640: 100%|██████████| 1/1 [00:29<00:00, 29.35s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.64s/it]\n",
            "                   all          4         17   0.000929        0.1     0.0028   0.000686\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     51/100         0G      1.781      3.587      2.072         40        640: 100%|██████████| 1/1 [00:29<00:00, 29.57s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.85s/it]\n",
            "                   all          4         17   0.000935        0.1    0.00279   0.000671\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     52/100         0G      1.944      3.619      2.216         16        640: 100%|██████████| 1/1 [00:30<00:00, 30.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.76s/it]\n",
            "                   all          4         17   0.000935        0.1    0.00279   0.000671\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     53/100         0G      1.746      3.599      2.171         29        640: 100%|██████████| 1/1 [00:29<00:00, 29.70s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.04s/it]\n",
            "                   all          4         17   0.000935        0.1    0.00279   0.000671\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     54/100         0G       2.05      3.534      2.148         25        640: 100%|██████████| 1/1 [00:30<00:00, 30.68s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.13s/it]\n",
            "                   all          4         17    0.00078     0.0833     0.0027   0.000634\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     55/100         0G      1.545      3.269      1.859         26        640: 100%|██████████| 1/1 [00:29<00:00, 29.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.73s/it]\n",
            "                   all          4         17    0.00078     0.0833     0.0027   0.000634\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     56/100         0G      1.783      3.301      2.063         31        640: 100%|██████████| 1/1 [00:30<00:00, 30.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.57s/it]\n",
            "                   all          4         17    0.00078     0.0833     0.0027   0.000634\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     57/100         0G      1.535      3.345      2.033         37        640: 100%|██████████| 1/1 [00:30<00:00, 30.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.24s/it]\n",
            "                   all          4         17   0.000786     0.0833    0.00289   0.000718\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     58/100         0G      1.978      3.733      2.273         33        640: 100%|██████████| 1/1 [00:31<00:00, 31.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.11s/it]\n",
            "                   all          4         17   0.000786     0.0833    0.00289   0.000718\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     59/100         0G      1.717      3.459       2.01         31        640: 100%|██████████| 1/1 [00:30<00:00, 30.30s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.62s/it]\n",
            "                   all          4         17   0.000786     0.0833    0.00289   0.000718\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     60/100         0G      1.868      3.274      2.059         17        640: 100%|██████████| 1/1 [00:31<00:00, 31.35s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.86s/it]\n",
            "                   all          4         17   0.000791     0.0833    0.00316   0.000751\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     61/100         0G      1.744       3.39      1.791         33        640: 100%|██████████| 1/1 [00:31<00:00, 31.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:13<00:00, 13.43s/it]\n",
            "                   all          4         17   0.000791     0.0833    0.00316   0.000751\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     62/100         0G      1.654       3.37      1.873         21        640: 100%|██████████| 1/1 [00:29<00:00, 29.44s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.33s/it]\n",
            "                   all          4         17   0.000791     0.0833    0.00316   0.000751\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     63/100         0G      1.541      3.338      1.743         24        640: 100%|██████████| 1/1 [00:30<00:00, 30.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.87s/it]\n",
            "                   all          4         17    0.00096        0.1    0.00352   0.000794\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     64/100         0G      1.858      3.388      2.096         33        640: 100%|██████████| 1/1 [00:29<00:00, 29.35s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.68s/it]\n",
            "                   all          4         17    0.00096        0.1    0.00352   0.000794\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     65/100         0G      1.686      3.223      1.884         32        640: 100%|██████████| 1/1 [00:29<00:00, 29.26s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.59s/it]\n",
            "                   all          4         17    0.00096        0.1    0.00352   0.000794\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     66/100         0G      1.493      3.155      1.797         37        640: 100%|██████████| 1/1 [00:30<00:00, 30.12s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.06s/it]\n",
            "                   all          4         17   0.000951        0.1    0.00359   0.000831\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     67/100         0G      1.754      3.347      2.001         35        640: 100%|██████████| 1/1 [00:29<00:00, 29.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "                   all          4         17   0.000951        0.1    0.00359   0.000831\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     68/100         0G      1.694      3.649      2.207         23        640: 100%|██████████| 1/1 [00:28<00:00, 28.84s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.52s/it]\n",
            "                   all          4         17   0.000951        0.1    0.00359   0.000831\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     69/100         0G      2.026      3.668      2.286         30        640: 100%|██████████| 1/1 [00:29<00:00, 29.52s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.22s/it]\n",
            "                   all          4         17    0.00093        0.1    0.00358   0.000713\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     70/100         0G      2.047      3.355      2.167         24        640: 100%|██████████| 1/1 [00:29<00:00, 29.52s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.86s/it]\n",
            "                   all          4         17    0.00093        0.1    0.00358   0.000713\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     71/100         0G      1.788      3.484      2.114         35        640: 100%|██████████| 1/1 [00:29<00:00, 29.92s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.03s/it]\n",
            "                   all          4         17    0.00093        0.1    0.00358   0.000713\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     72/100         0G      1.414       3.15      1.776         37        640: 100%|██████████| 1/1 [00:30<00:00, 30.33s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.93s/it]\n",
            "                   all          4         17   0.000938        0.1    0.00374   0.000743\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     73/100         0G      1.662      3.148      1.825         42        640: 100%|██████████| 1/1 [00:30<00:00, 30.42s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.07s/it]\n",
            "                   all          4         17   0.000938        0.1    0.00374   0.000743\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     74/100         0G      1.465      3.021      1.704         26        640: 100%|██████████| 1/1 [00:30<00:00, 30.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.94s/it]\n",
            "                   all          4         17   0.000938        0.1    0.00374   0.000743\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     75/100         0G      2.215      3.357      2.251         14        640: 100%|██████████| 1/1 [00:32<00:00, 32.54s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.91s/it]\n",
            "                   all          4         17   0.000945        0.1    0.00332   0.000667\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     76/100         0G      1.599      3.352      1.969         23        640: 100%|██████████| 1/1 [00:30<00:00, 30.47s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.75s/it]\n",
            "                   all          4         17   0.000945        0.1    0.00332   0.000667\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     77/100         0G      1.336      3.046       1.77         31        640: 100%|██████████| 1/1 [00:30<00:00, 30.35s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.42s/it]\n",
            "                   all          4         17   0.000945        0.1    0.00332   0.000667\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     78/100         0G      1.741      3.346      1.989         34        640: 100%|██████████| 1/1 [00:29<00:00, 29.94s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n",
            "                   all          4         17   0.000782     0.0833    0.00117   0.000437\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     79/100         0G       1.63      3.397      1.935         37        640: 100%|██████████| 1/1 [00:29<00:00, 29.81s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.82s/it]\n",
            "                   all          4         17   0.000782     0.0833    0.00117   0.000437\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     80/100         0G      1.724       3.23      1.776         30        640: 100%|██████████| 1/1 [00:29<00:00, 29.47s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.76s/it]\n",
            "                   all          4         17   0.000782     0.0833    0.00117   0.000437\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     81/100         0G      1.921      3.429      2.164         28        640: 100%|██████████| 1/1 [00:29<00:00, 29.89s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.95s/it]\n",
            "                   all          4         17   0.000754     0.0833    0.00129   0.000412\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     82/100         0G      1.536      2.998      1.934         35        640: 100%|██████████| 1/1 [00:29<00:00, 29.22s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.53s/it]\n",
            "                   all          4         17   0.000754     0.0833    0.00129   0.000412\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     83/100         0G      1.505      3.289      1.946         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.41s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.76s/it]\n",
            "                   all          4         17   0.000754     0.0833    0.00129   0.000412\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     84/100         0G      1.498        2.9       1.85         28        640: 100%|██████████| 1/1 [00:29<00:00, 29.92s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.01s/it]\n",
            "                   all          4         17   0.000764     0.0833    0.00147   0.000434\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     85/100         0G      1.544      3.161      1.778         50        640: 100%|██████████| 1/1 [00:29<00:00, 29.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.65s/it]\n",
            "                   all          4         17   0.000764     0.0833    0.00147   0.000434\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     86/100         0G      1.397      3.029      1.798         18        640: 100%|██████████| 1/1 [00:29<00:00, 29.41s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.59s/it]\n",
            "                   all          4         17   0.000764     0.0833    0.00147   0.000434\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     87/100         0G      1.321      3.029      1.726         32        640: 100%|██████████| 1/1 [00:29<00:00, 29.28s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.14s/it]\n",
            "                   all          4         17   0.000764     0.0833    0.00147   0.000434\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     88/100         0G      1.439      2.955      1.807         21        640: 100%|██████████| 1/1 [00:30<00:00, 30.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n",
            "                   all          4         17   0.000769     0.0833    0.00153   0.000442\n",
            "Stopping training early as no improvement observed in last 50 epochs. Best results observed at epoch 38, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=50) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "88 epochs completed in 1.116 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 87.8MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 87.8MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.194 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (AMD EPYC 7B12)\n",
            "Model summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.26s/it]\n",
            "                   all          4         17      0.167      0.233       0.17      0.117\n",
            "                person          4         10     0.0036        0.4     0.0247    0.00474\n",
            "                   dog          4          1          0          0          0          0\n",
            "                 horse          4          2          0          0          0          0\n",
            "              elephant          4          2          0          0          0          0\n",
            "              umbrella          4          1          1          1      0.995      0.697\n",
            "          potted plant          4          1          0          0          0          0\n",
            "Speed: 1.0ms preprocess, 2299.9ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model,\n",
        "    {torch.nn.Conv2d},\n",
        "    dtype=torch.qint8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(quantized_model.state_dict(), 'quantized.pt')"
      ],
      "metadata": {
        "id": "qcLytCgfi7my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqzPFYzrEHKC",
        "outputId": "fc6e0e90-e4dd-4954-814e-95d2a84020a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quantized_model size: 166.848MB\n"
          ]
        }
      ],
      "source": [
        "param_size = 0\n",
        "for param in quantized_model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in quantized_model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('quantized_model size: {:.3f}MB'.format(size_all_mb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "03f25e245ab943f6bbcc170c755f08cf",
            "4117c4fca1fb4920a985aec0352dce82",
            "12e4bab27848476c93fe4dbb0e51a7b5",
            "99aa29af46c94a1b8d58494482d405a5",
            "61387f4652bf44d681461f442c52b6d6",
            "9da178651a74454783a905f97a41957e",
            "e56ff4085f364394aa484784d9a345b8",
            "41688174dffd436a97e32fe800ec1286",
            "33d550664ccb49feb6005381aedfa1f1",
            "ea134ee2c9864857a0a0c4c172db2f42",
            "068090c470814ec6a57424f70b539985"
          ]
        },
        "id": "W6Mq58RJEH7d",
        "outputId": "5adfc76c-c5c4-47cc-b0af-804aa0fdee4c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03f25e245ab943f6bbcc170c755f08cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 417.3ms\n",
            "Speed: 0.0ms preprocess, 417.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 307.1ms\n",
            "Speed: 0.0ms preprocess, 307.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 299.1ms\n",
            "Speed: 0.0ms preprocess, 299.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 280.5ms\n",
            "Speed: 0.0ms preprocess, 280.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 282.1ms\n",
            "Speed: 0.0ms preprocess, 282.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 279.5ms\n",
            "Speed: 0.0ms preprocess, 279.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 292.0ms\n",
            "Speed: 0.0ms preprocess, 292.0ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 394.7ms\n",
            "Speed: 0.0ms preprocess, 394.7ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 410.1ms\n",
            "Speed: 0.0ms preprocess, 410.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 400.6ms\n",
            "Speed: 0.0ms preprocess, 400.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 405.3ms\n",
            "Speed: 0.0ms preprocess, 405.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 405.8ms\n",
            "Speed: 0.0ms preprocess, 405.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 334.9ms\n",
            "Speed: 0.0ms preprocess, 334.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 289.5ms\n",
            "Speed: 0.0ms preprocess, 289.5ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 286.8ms\n",
            "Speed: 0.0ms preprocess, 286.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 280.6ms\n",
            "Speed: 0.0ms preprocess, 280.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 291.0ms\n",
            "Speed: 0.0ms preprocess, 291.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 300.9ms\n",
            "Speed: 0.0ms preprocess, 300.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 284.8ms\n",
            "Speed: 0.0ms preprocess, 284.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 274.3ms\n",
            "Speed: 0.0ms preprocess, 274.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 284.9ms\n",
            "Speed: 0.0ms preprocess, 284.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 276.4ms\n",
            "Speed: 0.0ms preprocess, 276.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 276.6ms\n",
            "Speed: 0.0ms preprocess, 276.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 298.0ms\n",
            "Speed: 0.0ms preprocess, 298.0ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 301.2ms\n",
            "Speed: 0.0ms preprocess, 301.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 288.2ms\n",
            "Speed: 0.0ms preprocess, 288.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 280.1ms\n",
            "Speed: 0.0ms preprocess, 280.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 287.8ms\n",
            "Speed: 0.0ms preprocess, 287.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 281.2ms\n",
            "Speed: 0.2ms preprocess, 281.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 291.6ms\n",
            "Speed: 0.0ms preprocess, 291.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 299.5ms\n",
            "Speed: 0.0ms preprocess, 299.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 283.5ms\n",
            "Speed: 0.0ms preprocess, 283.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 281.2ms\n",
            "Speed: 0.0ms preprocess, 281.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 279.8ms\n",
            "Speed: 0.0ms preprocess, 279.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 292.2ms\n",
            "Speed: 0.0ms preprocess, 292.2ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.4ms\n",
            "Speed: 0.0ms preprocess, 303.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 284.2ms\n",
            "Speed: 0.0ms preprocess, 284.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 291.3ms\n",
            "Speed: 0.0ms preprocess, 291.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 287.3ms\n",
            "Speed: 0.0ms preprocess, 287.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 283.3ms\n",
            "Speed: 0.0ms preprocess, 283.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 312.7ms\n",
            "Speed: 0.0ms preprocess, 312.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 278.0ms\n",
            "Speed: 0.0ms preprocess, 278.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 291.7ms\n",
            "Speed: 0.0ms preprocess, 291.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 286.9ms\n",
            "Speed: 0.0ms preprocess, 286.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 299.9ms\n",
            "Speed: 0.0ms preprocess, 299.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 348.4ms\n",
            "Speed: 0.0ms preprocess, 348.4ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 396.8ms\n",
            "Speed: 0.0ms preprocess, 396.8ms inference, 2.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 403.1ms\n",
            "Speed: 0.0ms preprocess, 403.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 389.3ms\n",
            "Speed: 0.0ms preprocess, 389.3ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 385.4ms\n",
            "Speed: 0.0ms preprocess, 385.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 402.5ms\n",
            "Speed: 0.0ms preprocess, 402.5ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 287.4ms\n",
            "Speed: 0.0ms preprocess, 287.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 286.2ms\n",
            "Speed: 0.0ms preprocess, 286.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 305.5ms\n",
            "Speed: 0.0ms preprocess, 305.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 291.6ms\n",
            "Speed: 0.0ms preprocess, 291.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 292.6ms\n",
            "Speed: 0.0ms preprocess, 292.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 282.3ms\n",
            "Speed: 0.0ms preprocess, 282.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 296.3ms\n",
            "Speed: 0.0ms preprocess, 296.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 282.8ms\n",
            "Speed: 0.0ms preprocess, 282.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 291.6ms\n",
            "Speed: 0.0ms preprocess, 291.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 285.0ms\n",
            "Speed: 0.0ms preprocess, 285.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 286.7ms\n",
            "Speed: 0.0ms preprocess, 286.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 292.6ms\n",
            "Speed: 0.0ms preprocess, 292.6ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 279.4ms\n",
            "Speed: 0.0ms preprocess, 279.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 282.4ms\n",
            "Speed: 0.0ms preprocess, 282.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 277.9ms\n",
            "Speed: 0.0ms preprocess, 277.9ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 284.8ms\n",
            "Speed: 0.0ms preprocess, 284.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 297.1ms\n",
            "Speed: 0.0ms preprocess, 297.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 288.7ms\n",
            "Speed: 0.0ms preprocess, 288.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 294.3ms\n",
            "Speed: 0.0ms preprocess, 294.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 282.6ms\n",
            "Speed: 0.0ms preprocess, 282.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 284.5ms\n",
            "Speed: 0.0ms preprocess, 284.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 300.5ms\n",
            "Speed: 0.0ms preprocess, 300.5ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.6ms\n",
            "Speed: 0.0ms preprocess, 303.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 296.0ms\n",
            "Speed: 0.0ms preprocess, 296.0ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 283.4ms\n",
            "Speed: 0.0ms preprocess, 283.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 291.0ms\n",
            "Speed: 0.0ms preprocess, 291.0ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 285.1ms\n",
            "Speed: 0.0ms preprocess, 285.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 276.2ms\n",
            "Speed: 0.0ms preprocess, 276.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 306.7ms\n",
            "Speed: 0.0ms preprocess, 306.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 287.8ms\n",
            "Speed: 0.0ms preprocess, 287.8ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 294.1ms\n",
            "Speed: 0.0ms preprocess, 294.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.9ms\n",
            "Speed: 0.0ms preprocess, 303.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 314.0ms\n",
            "Speed: 0.1ms preprocess, 314.0ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 402.1ms\n",
            "Speed: 0.0ms preprocess, 402.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 411.2ms\n",
            "Speed: 0.0ms preprocess, 411.2ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 401.8ms\n",
            "Speed: 0.0ms preprocess, 401.8ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 399.3ms\n",
            "Speed: 0.0ms preprocess, 399.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 393.8ms\n",
            "Speed: 0.0ms preprocess, 393.8ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 304.2ms\n",
            "Speed: 0.0ms preprocess, 304.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 286.6ms\n",
            "Speed: 0.0ms preprocess, 286.6ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 286.3ms\n",
            "Speed: 0.0ms preprocess, 286.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 293.2ms\n",
            "Speed: 0.0ms preprocess, 293.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 278.5ms\n",
            "Speed: 0.0ms preprocess, 278.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 315.7ms\n",
            "Speed: 0.0ms preprocess, 315.7ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 281.1ms\n",
            "Speed: 0.0ms preprocess, 281.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 279.9ms\n",
            "Speed: 0.0ms preprocess, 279.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 298.1ms\n",
            "Speed: 0.0ms preprocess, 298.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 301.4ms\n",
            "Speed: 0.0ms preprocess, 301.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 299.6ms\n",
            "Speed: 0.0ms preprocess, 299.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg inference time: 322.8817 ms\n"
          ]
        }
      ],
      "source": [
        "inp = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "num_samples = 100\n",
        "start_time = time.time()\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    output = quantized_model(inp)\n",
        "end_time = time.time()\n",
        "\n",
        "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
        "print(f'Avg inference time: {infer_time:.4f} ms')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Прунинг"
      ],
      "metadata": {
        "id": "UdIrYqrZrcOc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifmcxX8REyLK"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils import prune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iaa-kQhhGBsO"
      },
      "outputs": [],
      "source": [
        "for name, m in model.named_modules():\n",
        "  params = list(m.named_parameters())\n",
        "  if len(params) and params[0][0] == 'weight':\n",
        "    prune.l1_unstructured(m, name=params[0][0], amount=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf0UxbbqKkAO",
        "outputId": "95f92bbc-3303-42ec-861e-50aa4fdf35d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 333.072MB\n"
          ]
        }
      ],
      "source": [
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f0d45a2eba8249db88bc191672234287",
            "c2da8e18cf544a13a7ee2c58eedd0095",
            "1e04e29ab5984dd0aee8b6749fb7a865",
            "bd8fbd93039a47058ff5797d3c10600c",
            "b38676f2990a4c18b42d653fbe56507c",
            "24aed408350f4151ab64a7c254c082e9",
            "fc1fe59c7c934b7aafd7e213aec66c65",
            "2352db1eb66f4d8b8a8063fab5142ca1",
            "d53891585823477c8ee3970e88391349",
            "872b588206804dd69e557a1703026ebc",
            "8aaf50a6867d4440945bf632150b0416"
          ]
        },
        "id": "i9r6ZqnjK23D",
        "outputId": "60608259-a1de-4eb3-a7f1-98c24b2838d2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0d45a2eba8249db88bc191672234287"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 224x224 (no detections), 354.1ms\n",
            "Speed: 0.0ms preprocess, 354.1ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 305.3ms\n",
            "Speed: 0.0ms preprocess, 305.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 307.9ms\n",
            "Speed: 0.0ms preprocess, 307.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 312.6ms\n",
            "Speed: 0.0ms preprocess, 312.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 307.9ms\n",
            "Speed: 0.0ms preprocess, 307.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 314.2ms\n",
            "Speed: 0.0ms preprocess, 314.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 318.7ms\n",
            "Speed: 0.0ms preprocess, 318.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 311.4ms\n",
            "Speed: 0.0ms preprocess, 311.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 311.3ms\n",
            "Speed: 0.0ms preprocess, 311.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 316.5ms\n",
            "Speed: 0.0ms preprocess, 316.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 317.6ms\n",
            "Speed: 0.0ms preprocess, 317.6ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 309.0ms\n",
            "Speed: 0.0ms preprocess, 309.0ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 324.3ms\n",
            "Speed: 0.0ms preprocess, 324.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 316.4ms\n",
            "Speed: 0.0ms preprocess, 316.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 333.7ms\n",
            "Speed: 0.0ms preprocess, 333.7ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 425.0ms\n",
            "Speed: 0.0ms preprocess, 425.0ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 416.7ms\n",
            "Speed: 0.0ms preprocess, 416.7ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 418.1ms\n",
            "Speed: 0.0ms preprocess, 418.1ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 413.3ms\n",
            "Speed: 0.0ms preprocess, 413.3ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 405.4ms\n",
            "Speed: 0.0ms preprocess, 405.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 317.7ms\n",
            "Speed: 0.0ms preprocess, 317.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 315.2ms\n",
            "Speed: 0.0ms preprocess, 315.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 313.5ms\n",
            "Speed: 0.0ms preprocess, 313.5ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 310.2ms\n",
            "Speed: 0.0ms preprocess, 310.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 316.3ms\n",
            "Speed: 0.0ms preprocess, 316.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 319.9ms\n",
            "Speed: 0.0ms preprocess, 319.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 316.4ms\n",
            "Speed: 0.0ms preprocess, 316.4ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 312.4ms\n",
            "Speed: 0.0ms preprocess, 312.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 315.6ms\n",
            "Speed: 0.0ms preprocess, 315.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 336.9ms\n",
            "Speed: 0.0ms preprocess, 336.9ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 325.6ms\n",
            "Speed: 0.0ms preprocess, 325.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 324.5ms\n",
            "Speed: 0.0ms preprocess, 324.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 311.4ms\n",
            "Speed: 0.0ms preprocess, 311.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 323.9ms\n",
            "Speed: 0.0ms preprocess, 323.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 314.5ms\n",
            "Speed: 0.0ms preprocess, 314.5ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 320.8ms\n",
            "Speed: 0.0ms preprocess, 320.8ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 314.3ms\n",
            "Speed: 0.0ms preprocess, 314.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 311.1ms\n",
            "Speed: 0.0ms preprocess, 311.1ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 324.5ms\n",
            "Speed: 0.0ms preprocess, 324.5ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 320.8ms\n",
            "Speed: 0.0ms preprocess, 320.8ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 321.9ms\n",
            "Speed: 0.0ms preprocess, 321.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 316.4ms\n",
            "Speed: 0.0ms preprocess, 316.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 317.5ms\n",
            "Speed: 0.0ms preprocess, 317.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 319.9ms\n",
            "Speed: 0.0ms preprocess, 319.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 311.4ms\n",
            "Speed: 0.0ms preprocess, 311.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 322.3ms\n",
            "Speed: 0.6ms preprocess, 322.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 314.3ms\n",
            "Speed: 0.0ms preprocess, 314.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 321.5ms\n",
            "Speed: 0.0ms preprocess, 321.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 323.8ms\n",
            "Speed: 0.0ms preprocess, 323.8ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 312.3ms\n",
            "Speed: 0.0ms preprocess, 312.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 402.3ms\n",
            "Speed: 0.0ms preprocess, 402.3ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 439.4ms\n",
            "Speed: 0.0ms preprocess, 439.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 445.8ms\n",
            "Speed: 0.0ms preprocess, 445.8ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 437.2ms\n",
            "Speed: 0.0ms preprocess, 437.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 434.9ms\n",
            "Speed: 0.0ms preprocess, 434.9ms inference, 1.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 318.0ms\n",
            "Speed: 0.0ms preprocess, 318.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 327.9ms\n",
            "Speed: 0.0ms preprocess, 327.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 321.2ms\n",
            "Speed: 0.0ms preprocess, 321.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 326.1ms\n",
            "Speed: 0.0ms preprocess, 326.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 322.2ms\n",
            "Speed: 0.0ms preprocess, 322.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 336.5ms\n",
            "Speed: 0.0ms preprocess, 336.5ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 319.3ms\n",
            "Speed: 0.0ms preprocess, 319.3ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 320.0ms\n",
            "Speed: 0.0ms preprocess, 320.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 310.6ms\n",
            "Speed: 0.0ms preprocess, 310.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 310.0ms\n",
            "Speed: 0.0ms preprocess, 310.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 323.4ms\n",
            "Speed: 0.0ms preprocess, 323.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 323.8ms\n",
            "Speed: 0.0ms preprocess, 323.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 313.7ms\n",
            "Speed: 0.0ms preprocess, 313.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 330.0ms\n",
            "Speed: 0.0ms preprocess, 330.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 329.3ms\n",
            "Speed: 0.0ms preprocess, 329.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 334.1ms\n",
            "Speed: 0.0ms preprocess, 334.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 322.0ms\n",
            "Speed: 0.0ms preprocess, 322.0ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 327.4ms\n",
            "Speed: 0.0ms preprocess, 327.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 320.3ms\n",
            "Speed: 0.0ms preprocess, 320.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 319.3ms\n",
            "Speed: 0.0ms preprocess, 319.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 319.0ms\n",
            "Speed: 0.0ms preprocess, 319.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 313.0ms\n",
            "Speed: 0.0ms preprocess, 313.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 330.3ms\n",
            "Speed: 0.0ms preprocess, 330.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 321.9ms\n",
            "Speed: 0.0ms preprocess, 321.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 319.8ms\n",
            "Speed: 0.0ms preprocess, 319.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 328.8ms\n",
            "Speed: 0.0ms preprocess, 328.8ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 320.6ms\n",
            "Speed: 0.0ms preprocess, 320.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 322.6ms\n",
            "Speed: 0.0ms preprocess, 322.6ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 320.3ms\n",
            "Speed: 0.0ms preprocess, 320.3ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 314.5ms\n",
            "Speed: 0.0ms preprocess, 314.5ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 410.8ms\n",
            "Speed: 0.0ms preprocess, 410.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 423.8ms\n",
            "Speed: 0.0ms preprocess, 423.8ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 440.9ms\n",
            "Speed: 0.0ms preprocess, 440.9ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 440.8ms\n",
            "Speed: 0.0ms preprocess, 440.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 429.7ms\n",
            "Speed: 0.0ms preprocess, 429.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 339.8ms\n",
            "Speed: 0.0ms preprocess, 339.8ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 330.6ms\n",
            "Speed: 0.0ms preprocess, 330.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 319.2ms\n",
            "Speed: 0.0ms preprocess, 319.2ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 318.4ms\n",
            "Speed: 0.0ms preprocess, 318.4ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 318.9ms\n",
            "Speed: 0.0ms preprocess, 318.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 322.6ms\n",
            "Speed: 0.0ms preprocess, 322.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 320.0ms\n",
            "Speed: 0.0ms preprocess, 320.0ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 315.9ms\n",
            "Speed: 0.0ms preprocess, 315.9ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 324.7ms\n",
            "Speed: 0.0ms preprocess, 324.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "0: 224x224 (no detections), 314.4ms\n",
            "Speed: 0.0ms preprocess, 314.4ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg inference time: 346.0605 ms\n"
          ]
        }
      ],
      "source": [
        "inp = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "num_samples = 100\n",
        "start_time = time.time()\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    output = model(inp / 255)\n",
        "end_time = time.time()\n",
        "\n",
        "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
        "print(f'Avg inference time: {infer_time:.4f} ms')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'pruned.pt')"
      ],
      "metadata": {
        "id": "PcawRsxejNqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j4hpOx0kkur",
        "outputId": "8d6f92b9-0f14-47cf-c06b-a658a3b20656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.194 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (AMD EPYC 7B12)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=coco8.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
            "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
            "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
            " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
            " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 22        [15, 18, 21]  1   5644480  ultralytics.nn.modules.head.Detect           [80, [256, 512, 512]]         \n",
            "Model summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs\n",
            "\n",
            "Transferred 6/595 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8/labels/train.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/100         0G      3.518      5.398      3.703         37        640: 100%|██████████| 1/1 [00:30<00:00, 30.69s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.79s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      2/100         0G      3.615      5.592      3.733         39        640: 100%|██████████| 1/1 [00:32<00:00, 32.70s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.22s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      3/100         0G       3.15      5.829      3.714         18        640: 100%|██████████| 1/1 [00:29<00:00, 29.64s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.50s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      4/100         0G      3.539      5.293      3.696         43        640: 100%|██████████| 1/1 [00:31<00:00, 31.31s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.78s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      5/100         0G      3.577      5.744      4.025         18        640: 100%|██████████| 1/1 [00:29<00:00, 29.52s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.53s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      6/100         0G      3.479      5.383      3.762         41        640: 100%|██████████| 1/1 [00:29<00:00, 29.69s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.02s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      7/100         0G      3.241      5.894      3.866         19        640: 100%|██████████| 1/1 [00:29<00:00, 29.21s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      8/100         0G      3.414      5.662      3.624         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.70s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      9/100         0G      3.339      5.411      3.816         36        640: 100%|██████████| 1/1 [00:30<00:00, 30.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.68s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     10/100         0G       3.36      5.759       3.86         16        640: 100%|██████████| 1/1 [00:29<00:00, 29.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.82s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     11/100         0G      3.641      6.042      3.874         13        640: 100%|██████████| 1/1 [00:28<00:00, 28.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.62s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     12/100         0G      3.601      5.516       3.75         36        640: 100%|██████████| 1/1 [00:30<00:00, 30.93s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.63s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     13/100         0G      3.237      5.414      3.666         31        640: 100%|██████████| 1/1 [00:29<00:00, 29.45s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:11<00:00, 11.75s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     14/100         0G       3.47      5.462      3.643         33        640: 100%|██████████| 1/1 [00:29<00:00, 29.72s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.55s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     15/100         0G      3.769      6.427      4.007         11        640: 100%|██████████| 1/1 [00:28<00:00, 28.93s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.88s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     16/100         0G      3.326      5.486      3.719         31        640: 100%|██████████| 1/1 [00:30<00:00, 30.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.95s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     17/100         0G      3.463      5.429      3.689         38        640: 100%|██████████| 1/1 [00:29<00:00, 29.53s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.86s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     18/100         0G      3.519      5.392      3.847         19        640: 100%|██████████| 1/1 [00:30<00:00, 30.18s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.90s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     19/100         0G      3.358      5.529      3.734         24        640: 100%|██████████| 1/1 [00:30<00:00, 30.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.65s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     20/100         0G      3.337      5.564      3.818         28        640: 100%|██████████| 1/1 [00:33<00:00, 33.77s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.19s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     21/100         0G      3.275      5.555      3.789         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.88s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.88s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     22/100         0G      3.523      5.493      3.851         32        640: 100%|██████████| 1/1 [00:30<00:00, 30.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.74s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     23/100         0G      3.554       5.38      3.749         41        640: 100%|██████████| 1/1 [00:29<00:00, 29.49s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     24/100         0G      3.476      5.486      3.815         29        640: 100%|██████████| 1/1 [00:28<00:00, 28.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.68s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     25/100         0G      3.258      5.713      3.796         29        640: 100%|██████████| 1/1 [00:28<00:00, 28.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.62s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     26/100         0G      3.636      5.451      3.704         32        640: 100%|██████████| 1/1 [00:29<00:00, 29.40s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.07s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     27/100         0G      3.649       5.42      3.667         32        640: 100%|██████████| 1/1 [00:28<00:00, 28.95s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.78s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     28/100         0G      3.154      5.677      3.685         16        640: 100%|██████████| 1/1 [00:29<00:00, 29.26s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     29/100         0G      3.389      5.849      3.841         15        640: 100%|██████████| 1/1 [00:29<00:00, 29.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.79s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     30/100         0G      3.727      5.423      3.828         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.41s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.01s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     31/100         0G      3.499      5.505        3.8         31        640: 100%|██████████| 1/1 [00:29<00:00, 29.38s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     32/100         0G      2.903      5.777      3.703         13        640: 100%|██████████| 1/1 [00:28<00:00, 28.96s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.74s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     33/100         0G      3.452      5.313      3.638         43        640: 100%|██████████| 1/1 [00:29<00:00, 29.48s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.72s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     34/100         0G      3.416      5.438      3.681         36        640: 100%|██████████| 1/1 [00:30<00:00, 30.62s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.68s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     35/100         0G      3.701      5.523       3.77         33        640: 100%|██████████| 1/1 [00:29<00:00, 29.21s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.31s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     36/100         0G      3.734      5.705       4.01         17        640: 100%|██████████| 1/1 [00:28<00:00, 28.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.53s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     37/100         0G      3.603       5.14      3.707         40        640: 100%|██████████| 1/1 [00:29<00:00, 29.66s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.19s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     38/100         0G      3.705      5.487      3.722         35        640: 100%|██████████| 1/1 [00:28<00:00, 28.47s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.74s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     39/100         0G      3.639      5.472      3.824         30        640: 100%|██████████| 1/1 [00:29<00:00, 29.02s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.20s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     40/100         0G      3.517      5.579      3.931         19        640: 100%|██████████| 1/1 [00:28<00:00, 28.28s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.69s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     41/100         0G      3.462      5.577      3.651         26        640: 100%|██████████| 1/1 [00:30<00:00, 30.21s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.43s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     42/100         0G      3.337      5.987      3.729         21        640: 100%|██████████| 1/1 [00:28<00:00, 28.49s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.48s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     43/100         0G       3.23      5.436      3.805         18        640: 100%|██████████| 1/1 [00:29<00:00, 29.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.81s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     44/100         0G      3.505      5.345      3.876         23        640: 100%|██████████| 1/1 [00:28<00:00, 28.61s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.71s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     45/100         0G      3.437      5.351      3.634         36        640: 100%|██████████| 1/1 [00:29<00:00, 29.62s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.77s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     46/100         0G      3.719      5.377      3.821         27        640: 100%|██████████| 1/1 [00:29<00:00, 29.61s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.12s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     47/100         0G      3.203      6.207      3.694         12        640: 100%|██████████| 1/1 [00:29<00:00, 29.90s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.95s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     48/100         0G      3.363      5.146      3.725         30        640: 100%|██████████| 1/1 [00:30<00:00, 30.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     49/100         0G      3.187       5.39      3.714         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.87s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.68s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     50/100         0G      3.127      5.297       3.68         33        640: 100%|██████████| 1/1 [00:29<00:00, 29.94s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.83s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     51/100         0G      3.429       5.24      3.584         40        640: 100%|██████████| 1/1 [00:29<00:00, 29.57s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.93s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     52/100         0G      3.496      5.656       3.88         16        640: 100%|██████████| 1/1 [00:28<00:00, 28.91s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.63s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     53/100         0G      3.419      5.522      3.652         29        640: 100%|██████████| 1/1 [00:29<00:00, 29.12s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.19s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     54/100         0G      3.627      5.395      3.863         25        640: 100%|██████████| 1/1 [00:29<00:00, 29.43s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:11<00:00, 11.02s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     55/100         0G      3.533      5.464      3.673         26        640: 100%|██████████| 1/1 [00:29<00:00, 29.77s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.67s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     56/100         0G       3.27      5.213      3.779         31        640: 100%|██████████| 1/1 [00:29<00:00, 29.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.50s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     57/100         0G      3.328      5.135      3.639         37        640: 100%|██████████| 1/1 [00:29<00:00, 29.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.83s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     58/100         0G      3.261      5.241      3.808         33        640: 100%|██████████| 1/1 [00:29<00:00, 29.35s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.86s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     59/100         0G      3.231      5.475      3.662         31        640: 100%|██████████| 1/1 [00:29<00:00, 29.33s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.89s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     60/100         0G      3.696       5.84      3.757         17        640: 100%|██████████| 1/1 [00:29<00:00, 29.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.31s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     61/100         0G      3.579      5.431      3.787         33        640: 100%|██████████| 1/1 [00:28<00:00, 28.84s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.88s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     62/100         0G      3.552        5.8      3.829         21        640: 100%|██████████| 1/1 [00:29<00:00, 29.35s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.92s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     63/100         0G      3.265      5.466      3.806         24        640: 100%|██████████| 1/1 [00:30<00:00, 30.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.86s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     64/100         0G      3.727      5.188      3.726         33        640: 100%|██████████| 1/1 [00:29<00:00, 29.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.89s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     65/100         0G      3.557      5.108      3.726         32        640: 100%|██████████| 1/1 [00:30<00:00, 30.26s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.93s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     66/100         0G      3.536      5.415      3.689         37        640: 100%|██████████| 1/1 [00:29<00:00, 29.37s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.46s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     67/100         0G      3.387      5.438      3.679         35        640: 100%|██████████| 1/1 [00:29<00:00, 29.51s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.88s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     68/100         0G      3.155      5.337       3.69         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.80s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     69/100         0G      3.591      5.311      3.859         30        640: 100%|██████████| 1/1 [00:31<00:00, 31.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.53s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     70/100         0G      3.649      5.344      3.794         24        640: 100%|██████████| 1/1 [00:32<00:00, 32.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.88s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     71/100         0G      3.565      5.333       3.72         35        640: 100%|██████████| 1/1 [00:29<00:00, 29.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     72/100         0G       3.28      5.587      3.588         37        640: 100%|██████████| 1/1 [00:30<00:00, 30.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:11<00:00, 11.84s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     73/100         0G      3.767      5.133      3.756         42        640: 100%|██████████| 1/1 [00:29<00:00, 29.73s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.76s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     74/100         0G      3.422      5.404      3.751         26        640: 100%|██████████| 1/1 [00:29<00:00, 29.73s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.16s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     75/100         0G      4.062      5.925      3.942         14        640: 100%|██████████| 1/1 [00:29<00:00, 29.94s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.91s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     76/100         0G       3.39      5.819      3.719         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.27s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     77/100         0G      3.365      5.068       3.81         31        640: 100%|██████████| 1/1 [00:29<00:00, 29.61s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     78/100         0G      3.445       5.22      3.777         34        640: 100%|██████████| 1/1 [00:29<00:00, 29.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     79/100         0G      3.428      5.172      3.689         37        640: 100%|██████████| 1/1 [00:29<00:00, 29.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     80/100         0G       3.55      5.368      3.812         30        640: 100%|██████████| 1/1 [00:29<00:00, 29.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.59s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     81/100         0G        3.5      5.397      3.689         28        640: 100%|██████████| 1/1 [00:28<00:00, 28.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.61s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     82/100         0G      3.284       5.28       3.83         35        640: 100%|██████████| 1/1 [00:29<00:00, 29.17s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.96s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     83/100         0G      3.373      5.606      3.545         23        640: 100%|██████████| 1/1 [00:28<00:00, 28.86s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.33s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     84/100         0G      3.632      5.268      3.816         28        640: 100%|██████████| 1/1 [00:29<00:00, 29.56s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     85/100         0G      3.451      5.099      3.664         50        640: 100%|██████████| 1/1 [00:28<00:00, 29.00s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.50s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     86/100         0G      3.399      5.508       3.89         18        640: 100%|██████████| 1/1 [00:28<00:00, 28.83s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.01s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     87/100         0G      3.511      5.402       3.58         32        640: 100%|██████████| 1/1 [00:28<00:00, 28.68s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     88/100         0G      3.292      5.313      3.824         21        640: 100%|██████████| 1/1 [00:29<00:00, 29.41s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.61s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     89/100         0G      3.496      5.543       3.79         19        640: 100%|██████████| 1/1 [00:29<00:00, 29.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.57s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     90/100         0G       3.53      5.371      3.761         36        640: 100%|██████████| 1/1 [00:28<00:00, 28.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.65s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     91/100         0G      3.065      5.696      3.724         13        640: 100%|██████████| 1/1 [00:31<00:00, 31.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.70s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     92/100         0G       3.27      5.675      3.741         13        640: 100%|██████████| 1/1 [00:28<00:00, 28.82s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     93/100         0G      3.035      5.526      3.817         13        640: 100%|██████████| 1/1 [00:29<00:00, 29.04s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.65s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     94/100         0G      3.647      6.114      3.702         13        640: 100%|██████████| 1/1 [00:28<00:00, 28.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     95/100         0G      3.228      5.409       3.73         13        640: 100%|██████████| 1/1 [00:28<00:00, 28.90s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.67s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     96/100         0G      3.221       5.73       3.69         13        640: 100%|██████████| 1/1 [00:29<00:00, 29.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.26s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     97/100         0G      3.198      5.397      3.713         13        640: 100%|██████████| 1/1 [00:28<00:00, 28.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.75s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     98/100         0G      2.815      5.426      3.655         13        640: 100%|██████████| 1/1 [00:29<00:00, 29.04s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.90s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     99/100         0G      3.025      5.271      3.677         13        640: 100%|██████████| 1/1 [00:29<00:00, 29.17s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.49s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "    100/100         0G      3.339      5.274      3.912         13        640: 100%|██████████| 1/1 [00:28<00:00, 28.96s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.67s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "100 epochs completed in 1.818 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 87.8MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 87.8MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.194 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (AMD EPYC 7B12)\n",
            "Model summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.62s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "Speed: 1.4ms preprocess, 2633.1ms inference, 0.0ms loss, 4.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: []\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x79bd2165e440>\n",
              "fitness: 0.0\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([], dtype=float64)\n",
              "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': 0.0, 'metrics/recall(B)': 0.0, 'metrics/mAP50(B)': 0.0, 'metrics/mAP50-95(B)': 0.0, 'fitness': 0.0}\n",
              "save_dir: PosixPath('runs/detect/train2')\n",
              "speed: {'preprocess': 1.3793110847473145, 'inference': 2633.0968737602234, 'loss': 0.00095367431640625, 'postprocess': 4.8416852951049805}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Кластеризация"
      ],
      "metadata": {
        "id": "0Lfi_hzCrgB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8l.pt')\n",
        "tensors = model.state_dict()"
      ],
      "metadata": {
        "id": "-34cDU_Fjvbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import kmeans_plusplus\n",
        "import numpy as np\n",
        "\n",
        "tensors = model.state_dict()\n",
        "for name in tensors:\n",
        "  if name.find('weight') < 0:\n",
        "    continue\n",
        "  n = np.prod(tensors[name].shape)\n",
        "  centroids = kmeans_plusplus(np.array(tensors[name].reshape(n, 1), dtype=float), 10)[0].squeeze()\n",
        "  arr = tensors[name].reshape(n,)\n",
        "  dist = abs(arr - centroids[0])\n",
        "  for i in range(1, len(centroids)):\n",
        "    dist = np.vstack((dist, abs(arr - centroids[i])))\n",
        "  args_of_cent = dist.argmin(axis = 0)\n",
        "  arr = centroids[args_of_cent]\n",
        "  tensors[name] = torch.Tensor(arr.reshape(tensors[name].shape))\n",
        "torch.save(tensors, 'clustered.pt')"
      ],
      "metadata": {
        "id": "2oB814mOj9oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8l.pt')\n",
        "model.load_state_dict(torch.load('clustered.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umt2Z7PIkHx_",
        "outputId": "0ac8460b-1ee6-4a2f-f9a4-5eee59fd4675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in model.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoHHhN2HkPc7",
        "outputId": "4e821d36-6017-4536-fa79-12c39dade330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 166.848MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "num_samples = 100\n",
        "start_time = time.time()\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    output = model(inp)\n",
        "end_time = time.time()\n",
        "\n",
        "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
        "print(f'Avg inference time: {infer_time:.4f} ms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6819600b694040d2accd3a2e9b9b8a8b",
            "15864e7b01a84f6a931b2520c2153ac1",
            "e8d3c8b4f3db433db7f6252ec4c63dca",
            "806025f2e75f497daaf1be7c3205ea60",
            "f8eedbabedf543e7a631fa4971046d64",
            "0c28a28082774b399d68578e25653559",
            "e45d63a9677c438d94de0eec39adc60d",
            "9d2b878464b7488085fd8daccdc1acab",
            "7e706b8aa57647038d6e124cb04ecfcd",
            "1327c835789f4b7a8202f3cdb30c60fc",
            "86c755ea68234f419f4a4d9269ff221f"
          ]
        },
        "id": "fFxhVzBekYLi",
        "outputId": "c188d4bd-0b19-496b-dce5-bde607260fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6819600b694040d2accd3a2e9b9b8a8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 309.3ms\n",
            "Speed: 0.0ms preprocess, 309.3ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 311.6ms\n",
            "Speed: 0.0ms preprocess, 311.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 298.8ms\n",
            "Speed: 0.0ms preprocess, 298.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 299.7ms\n",
            "Speed: 0.0ms preprocess, 299.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 296.6ms\n",
            "Speed: 0.0ms preprocess, 296.6ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 314.8ms\n",
            "Speed: 0.0ms preprocess, 314.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 310.9ms\n",
            "Speed: 0.0ms preprocess, 310.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 309.4ms\n",
            "Speed: 0.0ms preprocess, 309.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 308.4ms\n",
            "Speed: 0.0ms preprocess, 308.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 305.3ms\n",
            "Speed: 0.0ms preprocess, 305.3ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 296.0ms\n",
            "Speed: 0.0ms preprocess, 296.0ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 306.4ms\n",
            "Speed: 0.0ms preprocess, 306.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 301.0ms\n",
            "Speed: 0.0ms preprocess, 301.0ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 296.3ms\n",
            "Speed: 0.0ms preprocess, 296.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.2ms\n",
            "Speed: 0.0ms preprocess, 303.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 312.8ms\n",
            "Speed: 0.0ms preprocess, 312.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 400.1ms\n",
            "Speed: 0.0ms preprocess, 400.1ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 413.5ms\n",
            "Speed: 0.0ms preprocess, 413.5ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 409.5ms\n",
            "Speed: 0.0ms preprocess, 409.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 407.6ms\n",
            "Speed: 0.0ms preprocess, 407.6ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 444.9ms\n",
            "Speed: 0.0ms preprocess, 444.9ms inference, 1.5ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 308.1ms\n",
            "Speed: 0.0ms preprocess, 308.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 298.2ms\n",
            "Speed: 0.0ms preprocess, 298.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 302.7ms\n",
            "Speed: 0.0ms preprocess, 302.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 294.4ms\n",
            "Speed: 0.0ms preprocess, 294.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 301.9ms\n",
            "Speed: 0.0ms preprocess, 301.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 311.8ms\n",
            "Speed: 0.0ms preprocess, 311.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 315.3ms\n",
            "Speed: 0.0ms preprocess, 315.3ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 311.6ms\n",
            "Speed: 0.0ms preprocess, 311.6ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.0ms\n",
            "Speed: 0.0ms preprocess, 303.0ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 305.1ms\n",
            "Speed: 0.0ms preprocess, 305.1ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.7ms\n",
            "Speed: 0.0ms preprocess, 303.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 297.7ms\n",
            "Speed: 0.0ms preprocess, 297.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 288.5ms\n",
            "Speed: 0.0ms preprocess, 288.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.5ms\n",
            "Speed: 0.0ms preprocess, 303.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 298.7ms\n",
            "Speed: 0.0ms preprocess, 298.7ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.9ms\n",
            "Speed: 0.0ms preprocess, 303.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 300.6ms\n",
            "Speed: 0.0ms preprocess, 300.6ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.9ms\n",
            "Speed: 0.0ms preprocess, 303.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 316.6ms\n",
            "Speed: 0.0ms preprocess, 316.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 308.9ms\n",
            "Speed: 0.0ms preprocess, 308.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 306.2ms\n",
            "Speed: 0.0ms preprocess, 306.2ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 308.1ms\n",
            "Speed: 0.0ms preprocess, 308.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 294.5ms\n",
            "Speed: 0.0ms preprocess, 294.5ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 304.9ms\n",
            "Speed: 0.0ms preprocess, 304.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 315.4ms\n",
            "Speed: 0.0ms preprocess, 315.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 308.1ms\n",
            "Speed: 0.0ms preprocess, 308.1ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 309.1ms\n",
            "Speed: 0.0ms preprocess, 309.1ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 319.7ms\n",
            "Speed: 0.0ms preprocess, 319.7ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 310.9ms\n",
            "Speed: 0.0ms preprocess, 310.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 304.7ms\n",
            "Speed: 0.0ms preprocess, 304.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 312.5ms\n",
            "Speed: 0.0ms preprocess, 312.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 340.7ms\n",
            "Speed: 0.0ms preprocess, 340.7ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 416.1ms\n",
            "Speed: 0.0ms preprocess, 416.1ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 441.8ms\n",
            "Speed: 0.0ms preprocess, 441.8ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 453.1ms\n",
            "Speed: 0.0ms preprocess, 453.1ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 425.6ms\n",
            "Speed: 0.0ms preprocess, 425.6ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 356.6ms\n",
            "Speed: 0.0ms preprocess, 356.6ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 309.6ms\n",
            "Speed: 0.0ms preprocess, 309.6ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 317.4ms\n",
            "Speed: 0.0ms preprocess, 317.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 308.2ms\n",
            "Speed: 0.0ms preprocess, 308.2ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 313.5ms\n",
            "Speed: 0.0ms preprocess, 313.5ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 299.7ms\n",
            "Speed: 0.0ms preprocess, 299.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 300.3ms\n",
            "Speed: 0.0ms preprocess, 300.3ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 299.3ms\n",
            "Speed: 0.0ms preprocess, 299.3ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.5ms\n",
            "Speed: 0.2ms preprocess, 303.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 311.2ms\n",
            "Speed: 0.0ms preprocess, 311.2ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 313.7ms\n",
            "Speed: 0.0ms preprocess, 313.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 301.9ms\n",
            "Speed: 0.0ms preprocess, 301.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 314.4ms\n",
            "Speed: 0.0ms preprocess, 314.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 302.4ms\n",
            "Speed: 0.0ms preprocess, 302.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 324.5ms\n",
            "Speed: 0.1ms preprocess, 324.5ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 314.7ms\n",
            "Speed: 0.0ms preprocess, 314.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 320.9ms\n",
            "Speed: 0.0ms preprocess, 320.9ms inference, 1.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 302.4ms\n",
            "Speed: 0.0ms preprocess, 302.4ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.2ms\n",
            "Speed: 0.0ms preprocess, 303.2ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 307.9ms\n",
            "Speed: 0.0ms preprocess, 307.9ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.5ms\n",
            "Speed: 0.0ms preprocess, 303.5ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 302.7ms\n",
            "Speed: 0.0ms preprocess, 302.7ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 299.7ms\n",
            "Speed: 0.0ms preprocess, 299.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 298.5ms\n",
            "Speed: 0.0ms preprocess, 298.5ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 295.7ms\n",
            "Speed: 0.0ms preprocess, 295.7ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 299.6ms\n",
            "Speed: 0.0ms preprocess, 299.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 292.8ms\n",
            "Speed: 0.0ms preprocess, 292.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 300.8ms\n",
            "Speed: 0.0ms preprocess, 300.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 292.5ms\n",
            "Speed: 0.0ms preprocess, 292.5ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 289.1ms\n",
            "Speed: 0.0ms preprocess, 289.1ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 304.8ms\n",
            "Speed: 0.0ms preprocess, 304.8ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 335.3ms\n",
            "Speed: 0.0ms preprocess, 335.3ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 406.2ms\n",
            "Speed: 0.0ms preprocess, 406.2ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 406.0ms\n",
            "Speed: 0.0ms preprocess, 406.0ms inference, 1.2ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 402.9ms\n",
            "Speed: 0.0ms preprocess, 402.9ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 406.8ms\n",
            "Speed: 0.0ms preprocess, 406.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 385.9ms\n",
            "Speed: 0.0ms preprocess, 385.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 311.6ms\n",
            "Speed: 0.0ms preprocess, 311.6ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 296.1ms\n",
            "Speed: 0.0ms preprocess, 296.1ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 307.8ms\n",
            "Speed: 0.0ms preprocess, 307.8ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 303.3ms\n",
            "Speed: 0.0ms preprocess, 303.3ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 316.9ms\n",
            "Speed: 0.0ms preprocess, 316.9ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.481132507324219. Dividing input by 255.\n",
            "0: 224x224 (no detections), 305.6ms\n",
            "Speed: 0.0ms preprocess, 305.6ms inference, 0.8ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg inference time: 353.7584 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnvZbWnHkb1k",
        "outputId": "95d971e1-1289-434c-9bad-fe5c9a9dd854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.194 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (AMD EPYC 7B12)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=coco8.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
            "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
            "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
            " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
            " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
            " 22        [15, 18, 21]  1   5644480  ultralytics.nn.modules.head.Detect           [80, [256, 512, 512]]         \n",
            "Model summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs\n",
            "\n",
            "Transferred 110/595 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco8/labels/train.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      1/100         0G      2.896      5.312      3.009         37        640: 100%|██████████| 1/1 [00:29<00:00, 29.21s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.71s/it]\n",
            "                   all          4         17          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      2/100         0G      2.986      5.494      3.156         39        640: 100%|██████████| 1/1 [00:29<00:00, 29.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.79s/it]\n",
            "                   all          4         17    0.00231     0.0167    0.00211   0.000211\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      3/100         0G      3.113      5.775      3.409         18        640: 100%|██████████| 1/1 [00:28<00:00, 28.92s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.63s/it]\n",
            "                   all          4         17    0.00238     0.0167    0.00135   0.000541\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      4/100         0G       3.09      5.266      3.193         43        640: 100%|██████████| 1/1 [00:30<00:00, 30.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.25s/it]\n",
            "                   all          4         17    0.00238     0.0167    0.00135   0.000541\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      5/100         0G      2.796      5.686      3.077         18        640: 100%|██████████| 1/1 [00:29<00:00, 29.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.17s/it]\n",
            "                   all          4         17    0.00358     0.0333    0.00551     0.0011\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      6/100         0G      3.071      5.266      3.143         41        640: 100%|██████████| 1/1 [00:29<00:00, 29.60s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.60s/it]\n",
            "                   all          4         17    0.00147     0.0167   0.000845   8.45e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      7/100         0G      3.288      5.654      3.486         19        640: 100%|██████████| 1/1 [00:29<00:00, 29.47s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.65s/it]\n",
            "                   all          4         17    0.00147     0.0167   0.000845   8.45e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      8/100         0G      2.625       5.52      2.949         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.35s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.60s/it]\n",
            "                   all          4         17    0.00147     0.0167   0.000845   8.45e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      9/100         0G        2.7      5.258      2.949         36        640: 100%|██████████| 1/1 [00:29<00:00, 29.33s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.66s/it]\n",
            "                   all          4         17    0.00147     0.0167   0.000845   8.45e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     10/100         0G      3.003      5.505      3.285         16        640: 100%|██████████| 1/1 [00:29<00:00, 29.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.69s/it]\n",
            "                   all          4         17    0.00147     0.0167   0.000845   8.45e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     11/100         0G      3.053      5.683      3.191         13        640: 100%|██████████| 1/1 [00:29<00:00, 29.19s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.78s/it]\n",
            "                   all          4         17    0.00147     0.0167   0.000845   8.45e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     12/100         0G      3.007      5.281       3.09         36        640: 100%|██████████| 1/1 [00:29<00:00, 29.40s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "                   all          4         17    0.00147     0.0167   0.000845   8.45e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     13/100         0G      2.554       5.05       2.91         31        640: 100%|██████████| 1/1 [00:29<00:00, 29.90s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.66s/it]\n",
            "                   all          4         17   0.000992     0.0167   0.000613   0.000184\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     14/100         0G      2.746       5.09      2.786         33        640: 100%|██████████| 1/1 [00:29<00:00, 29.25s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.59s/it]\n",
            "                   all          4         17    0.00103     0.0167   0.000854   0.000256\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     15/100         0G      2.522      5.968      2.942         11        640: 100%|██████████| 1/1 [00:28<00:00, 28.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.55s/it]\n",
            "                   all          4         17   0.000942     0.0167   0.000711   0.000142\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     16/100         0G      2.861      4.977      3.066         31        640: 100%|██████████| 1/1 [00:29<00:00, 29.25s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000779   7.79e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     17/100         0G      2.795      5.103      2.946         38        640: 100%|██████████| 1/1 [00:29<00:00, 29.31s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.57s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     18/100         0G      2.805      5.112      2.969         19        640: 100%|██████████| 1/1 [00:32<00:00, 32.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.80s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     19/100         0G      2.678      5.312      2.909         24        640: 100%|██████████| 1/1 [00:29<00:00, 29.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.26s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     20/100         0G      2.305      4.967      2.765         28        640: 100%|██████████| 1/1 [00:29<00:00, 29.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.70s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     21/100         0G      2.316      4.846      2.678         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.36s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.69s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     22/100         0G      2.791      5.233      2.862         32        640: 100%|██████████| 1/1 [00:29<00:00, 29.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     23/100         0G      2.738      4.743      2.675         41        640: 100%|██████████| 1/1 [00:29<00:00, 29.53s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.60s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     24/100         0G      2.797      4.991      2.944         29        640: 100%|██████████| 1/1 [00:29<00:00, 29.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     25/100         0G      2.439      5.083      2.747         29        640: 100%|██████████| 1/1 [00:29<00:00, 29.30s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.69s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     26/100         0G      2.587       4.91      2.578         32        640: 100%|██████████| 1/1 [00:29<00:00, 29.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.61s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     27/100         0G      2.603      4.741      2.707         32        640: 100%|██████████| 1/1 [00:29<00:00, 29.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     28/100         0G      1.855      4.568      2.477         16        640: 100%|██████████| 1/1 [00:29<00:00, 29.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.50s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     29/100         0G      2.313       5.11      2.843         15        640: 100%|██████████| 1/1 [00:28<00:00, 28.95s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     30/100         0G      2.411      4.605      2.493         23        640: 100%|██████████| 1/1 [00:29<00:00, 29.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.52s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     31/100         0G      2.237      4.728      2.439         31        640: 100%|██████████| 1/1 [00:28<00:00, 28.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.56s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     32/100         0G      2.112      4.609      2.509         13        640: 100%|██████████| 1/1 [00:28<00:00, 28.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.12s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     33/100         0G      2.208      4.523      2.477         43        640: 100%|██████████| 1/1 [00:28<00:00, 28.75s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:11<00:00, 11.78s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     34/100         0G      2.518      4.637      2.651         36        640: 100%|██████████| 1/1 [00:28<00:00, 28.31s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.11s/it]\n",
            "                   all          4         17   0.000864     0.0167   0.000761   7.61e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     35/100         0G      2.391       4.72      2.632         33        640: 100%|██████████| 1/1 [00:28<00:00, 28.51s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.38s/it]\n",
            "                   all          4         17   0.000336     0.0167   0.000251   5.02e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     36/100         0G      2.627      5.215      2.631         17        640: 100%|██████████| 1/1 [00:27<00:00, 27.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.31s/it]\n",
            "                   all          4         17   0.000336     0.0167   0.000251   5.02e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     37/100         0G      2.551      4.464      2.546         40        640: 100%|██████████| 1/1 [00:28<00:00, 28.67s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "                   all          4         17   0.000332     0.0167   0.000444   0.000107\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     38/100         0G      2.418      4.551      2.522         35        640: 100%|██████████| 1/1 [00:28<00:00, 28.17s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.32s/it]\n",
            "                   all          4         17   0.000332     0.0167   0.000444   0.000107\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     39/100         0G      2.358      4.563      2.577         30        640: 100%|██████████| 1/1 [00:28<00:00, 28.37s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.44s/it]\n",
            "                   all          4         17   0.000342     0.0167   0.000292   0.000113\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     40/100         0G       2.07      4.211      2.339         19        640: 100%|██████████| 1/1 [00:28<00:00, 28.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.20s/it]\n",
            "                   all          4         17   0.000342     0.0167   0.000292   0.000113\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     41/100         0G      2.295      4.593      2.398         26        640: 100%|██████████| 1/1 [00:28<00:00, 28.47s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.14s/it]\n",
            "                   all          4         17   0.000354     0.0167   0.000304   0.000103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     42/100         0G      2.237      4.538      2.645         21        640: 100%|██████████| 1/1 [00:28<00:00, 28.68s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.57s/it]\n",
            "                   all          4         17   0.000354     0.0167   0.000304   0.000103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     43/100         0G      1.981      4.492      2.341         18        640: 100%|██████████| 1/1 [00:29<00:00, 29.02s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.14s/it]\n",
            "                   all          4         17   0.000383     0.0167   0.000311   0.000108\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     44/100         0G      2.497      4.239      2.585         23        640: 100%|██████████| 1/1 [00:31<00:00, 31.34s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.48s/it]\n",
            "                   all          4         17   0.000383     0.0167   0.000311   0.000108\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     45/100         0G       2.31      4.415      2.443         36        640: 100%|██████████| 1/1 [00:28<00:00, 28.75s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.30s/it]\n",
            "                   all          4         17   0.000398     0.0167   0.000256   0.000128\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     46/100         0G      2.334      4.256      2.535         27        640: 100%|██████████| 1/1 [00:28<00:00, 28.70s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.67s/it]\n",
            "                   all          4         17   0.000398     0.0167   0.000256   0.000128\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     47/100         0G      1.998      4.552      2.418         12        640: 100%|██████████| 1/1 [00:28<00:00, 28.82s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.24s/it]\n",
            "                   all          4         17   0.000403     0.0167   0.000248   4.96e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     48/100         0G      2.225      4.282      2.532         30        640: 100%|██████████| 1/1 [00:28<00:00, 28.48s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.52s/it]\n",
            "                   all          4         17   0.000403     0.0167   0.000248   4.96e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     49/100         0G      2.244      4.213      2.602         23        640: 100%|██████████| 1/1 [00:28<00:00, 28.50s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.39s/it]\n",
            "                   all          4         17   0.000403     0.0167   0.000237   4.73e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     50/100         0G      2.219      4.233      2.522         33        640: 100%|██████████| 1/1 [00:28<00:00, 28.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.51s/it]\n",
            "                   all          4         17   0.000403     0.0167   0.000237   4.73e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     51/100         0G      2.149      4.177      2.416         40        640: 100%|██████████| 1/1 [00:28<00:00, 28.81s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.63s/it]\n",
            "                   all          4         17   0.000403     0.0167   0.000237   4.73e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     52/100         0G       2.16      4.044      2.439         16        640: 100%|██████████| 1/1 [00:29<00:00, 29.06s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.63s/it]\n",
            "                   all          4         17   0.000403     0.0167   0.000237   4.73e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     53/100         0G      2.198      4.258      2.515         29        640: 100%|██████████| 1/1 [00:28<00:00, 28.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.60s/it]\n",
            "                   all          4         17   0.000403     0.0167   0.000237   4.73e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     54/100         0G      2.307      4.091       2.31         25        640: 100%|██████████| 1/1 [00:28<00:00, 28.96s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.28s/it]\n",
            "                   all          4         17   0.000403     0.0167   0.000237   4.73e-05\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "     55/100         0G      1.912      3.828      2.151         26        640: 100%|██████████| 1/1 [00:29<00:00, 29.48s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.65s/it]\n",
            "                   all          4         17   0.000403     0.0167   0.000237   4.73e-05\n",
            "Stopping training early as no improvement observed in last 50 epochs. Best results observed at epoch 5, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=50) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "55 epochs completed in 0.801 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 87.8MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 87.8MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.194 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (AMD EPYC 7B12)\n",
            "Model summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "                   all          4         17    0.00351     0.0333    0.00575    0.00115\n",
            "                person          4         10     0.0211        0.2     0.0345    0.00691\n",
            "                   dog          4          1          0          0          0          0\n",
            "                 horse          4          2          0          0          0          0\n",
            "              elephant          4          2          0          0          0          0\n",
            "              umbrella          4          1          0          0          0          0\n",
            "          potted plant          4          1          0          0          0          0\n",
            "Speed: 1.4ms preprocess, 2339.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
              "\n",
              "ap_class_index: array([ 0, 16, 17, 20, 25, 58])\n",
              "box: ultralytics.utils.metrics.Metric object\n",
              "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x79bd2274c160>\n",
              "fitness: 0.0016112280701754385\n",
              "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
              "maps: array([  0.0069053,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,           0,           0,   0.0011509,   0.0011509,           0,   0.0011509,   0.0011509,   0.0011509,\n",
              "         0.0011509,           0,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,\n",
              "         0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,           0,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,\n",
              "         0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509,   0.0011509])\n",
              "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
              "plot: True\n",
              "results_dict: {'metrics/precision(B)': 0.0035087719298245615, 'metrics/recall(B)': 0.03333333333333333, 'metrics/mAP50(B)': 0.00575438596491228, 'metrics/mAP50-95(B)': 0.001150877192982456, 'fitness': 0.0016112280701754385}\n",
              "save_dir: PosixPath('runs/detect/train3')\n",
              "speed: {'preprocess': 1.3719797134399414, 'inference': 2339.304745197296, 'loss': 0.0008940696716308594, 'postprocess': 1.0052919387817383}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Дистилляция"
      ],
      "metadata": {
        "id": "gLvKBQBPrlRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "autodistill \\\n",
        "autodistill-yolov8 \\\n",
        "roboflow \\\n",
        "supervision==0.9.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5iYFEqxLPxp",
        "outputId": "0a04d396-2811-4f0f-e169-4ceda67e76be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.0/527.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "from tqdm.notebook import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "VdtRuwVwLnz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/images\n",
        "%mkdir /content/images/train\n",
        "%mkdir /content/images/val\n",
        "\n",
        "%mkdir /content/labels\n",
        "%mkdir /content/labels/train\n",
        "%mkdir /content/labels/val"
      ],
      "metadata": {
        "id": "9he9a3kdN0Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow"
      ],
      "metadata": {
        "id": "AS9sMgTMOaHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = Roboflow(api_key=\"KPdQzbniHwGMSQyhZx76\")\n",
        "project = rf.workspace(\"surfacedefectdetectiondataset\").project(\"animal-mflmf\")\n",
        "dataset = project.version(1).download(\"yolov8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovEmqWx5OE_p",
        "outputId": "e02e29c7-f8e2-4796-8355-a84764776acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dependency ultralytics==8.0.134 is required but found version=8.0.195, to fix: `pip install ultralytics==8.0.134`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in animal-1 to yolov8:: 100%|██████████| 30524/30524 [00:00<00:00, 38485.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to animal-1 in yolov8:: 100%|██████████| 1536/1536 [00:00<00:00, 4439.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9yprgDJOgsP",
        "outputId": "0ff61401-efac-4ca6-90d1-357324c4c3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/cat_dog_dataset/ /content/animal-1/train/"
      ],
      "metadata": {
        "id": "r0QHJEckb5Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_model = YOLO(\"yolov8l.pt\")\n",
        "target_model.train(data = '/content/animal-1/data.yaml', epochs=15, batch = 32, pretrained = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "2BZsmtMaNnfg",
        "outputId": "08a5a76c-37da-44e8-eca9-0363e8e5bb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.195 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=/content/animal-1/data.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train9\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f95d4d3f873b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov8l.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/animal-1/data.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/callbacks/base.py\u001b[0m in \u001b[0;36madd_integration_callbacks\u001b[0;34m(instance)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics.utils.callbacks.hub'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48d5c76fe9b04937bf110f0649a53651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_978e791731214ca68096d0666658312e",
              "IPY_MODEL_c7001d56b4de4dfc88a22413fe531d7a",
              "IPY_MODEL_9b91b3aa90094e179703b93095af60dd"
            ],
            "layout": "IPY_MODEL_85a8b3a8910d4cc3bf8456536a68d5f1"
          }
        },
        "978e791731214ca68096d0666658312e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b04c77be194449f1b44a57565be3b706",
            "placeholder": "​",
            "style": "IPY_MODEL_d2afef5ef82340ecb172a158327cb178",
            "value": "100%"
          }
        },
        "c7001d56b4de4dfc88a22413fe531d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_773899e4716842539f3f4fa83b4d4030",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ac34fc27d3c4f06ab78be2c34ab7612",
            "value": 100
          }
        },
        "9b91b3aa90094e179703b93095af60dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66713c26ca174271a4f0dabeea233768",
            "placeholder": "​",
            "style": "IPY_MODEL_dacf824c34a349c28ebf563c578f0f63",
            "value": " 100/100 [00:37&lt;00:00,  2.56it/s]"
          }
        },
        "85a8b3a8910d4cc3bf8456536a68d5f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b04c77be194449f1b44a57565be3b706": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2afef5ef82340ecb172a158327cb178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "773899e4716842539f3f4fa83b4d4030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ac34fc27d3c4f06ab78be2c34ab7612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66713c26ca174271a4f0dabeea233768": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dacf824c34a349c28ebf563c578f0f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03f25e245ab943f6bbcc170c755f08cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4117c4fca1fb4920a985aec0352dce82",
              "IPY_MODEL_12e4bab27848476c93fe4dbb0e51a7b5",
              "IPY_MODEL_99aa29af46c94a1b8d58494482d405a5"
            ],
            "layout": "IPY_MODEL_61387f4652bf44d681461f442c52b6d6"
          }
        },
        "4117c4fca1fb4920a985aec0352dce82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9da178651a74454783a905f97a41957e",
            "placeholder": "​",
            "style": "IPY_MODEL_e56ff4085f364394aa484784d9a345b8",
            "value": "100%"
          }
        },
        "12e4bab27848476c93fe4dbb0e51a7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41688174dffd436a97e32fe800ec1286",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33d550664ccb49feb6005381aedfa1f1",
            "value": 100
          }
        },
        "99aa29af46c94a1b8d58494482d405a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea134ee2c9864857a0a0c4c172db2f42",
            "placeholder": "​",
            "style": "IPY_MODEL_068090c470814ec6a57424f70b539985",
            "value": " 100/100 [00:32&lt;00:00,  3.24it/s]"
          }
        },
        "61387f4652bf44d681461f442c52b6d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9da178651a74454783a905f97a41957e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e56ff4085f364394aa484784d9a345b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41688174dffd436a97e32fe800ec1286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d550664ccb49feb6005381aedfa1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea134ee2c9864857a0a0c4c172db2f42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "068090c470814ec6a57424f70b539985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0d45a2eba8249db88bc191672234287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2da8e18cf544a13a7ee2c58eedd0095",
              "IPY_MODEL_1e04e29ab5984dd0aee8b6749fb7a865",
              "IPY_MODEL_bd8fbd93039a47058ff5797d3c10600c"
            ],
            "layout": "IPY_MODEL_b38676f2990a4c18b42d653fbe56507c"
          }
        },
        "c2da8e18cf544a13a7ee2c58eedd0095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24aed408350f4151ab64a7c254c082e9",
            "placeholder": "​",
            "style": "IPY_MODEL_fc1fe59c7c934b7aafd7e213aec66c65",
            "value": "100%"
          }
        },
        "1e04e29ab5984dd0aee8b6749fb7a865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2352db1eb66f4d8b8a8063fab5142ca1",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d53891585823477c8ee3970e88391349",
            "value": 100
          }
        },
        "bd8fbd93039a47058ff5797d3c10600c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_872b588206804dd69e557a1703026ebc",
            "placeholder": "​",
            "style": "IPY_MODEL_8aaf50a6867d4440945bf632150b0416",
            "value": " 100/100 [00:34&lt;00:00,  3.02it/s]"
          }
        },
        "b38676f2990a4c18b42d653fbe56507c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24aed408350f4151ab64a7c254c082e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc1fe59c7c934b7aafd7e213aec66c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2352db1eb66f4d8b8a8063fab5142ca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d53891585823477c8ee3970e88391349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "872b588206804dd69e557a1703026ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aaf50a6867d4440945bf632150b0416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6819600b694040d2accd3a2e9b9b8a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15864e7b01a84f6a931b2520c2153ac1",
              "IPY_MODEL_e8d3c8b4f3db433db7f6252ec4c63dca",
              "IPY_MODEL_806025f2e75f497daaf1be7c3205ea60"
            ],
            "layout": "IPY_MODEL_f8eedbabedf543e7a631fa4971046d64"
          }
        },
        "15864e7b01a84f6a931b2520c2153ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c28a28082774b399d68578e25653559",
            "placeholder": "​",
            "style": "IPY_MODEL_e45d63a9677c438d94de0eec39adc60d",
            "value": "100%"
          }
        },
        "e8d3c8b4f3db433db7f6252ec4c63dca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d2b878464b7488085fd8daccdc1acab",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e706b8aa57647038d6e124cb04ecfcd",
            "value": 100
          }
        },
        "806025f2e75f497daaf1be7c3205ea60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1327c835789f4b7a8202f3cdb30c60fc",
            "placeholder": "​",
            "style": "IPY_MODEL_86c755ea68234f419f4a4d9269ff221f",
            "value": " 100/100 [00:35&lt;00:00,  3.05it/s]"
          }
        },
        "f8eedbabedf543e7a631fa4971046d64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c28a28082774b399d68578e25653559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e45d63a9677c438d94de0eec39adc60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d2b878464b7488085fd8daccdc1acab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e706b8aa57647038d6e124cb04ecfcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1327c835789f4b7a8202f3cdb30c60fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86c755ea68234f419f4a4d9269ff221f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}