{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b26dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e71d106d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2a3f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worker/miniconda3/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from optimum.intel import OVModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd269fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using framework PyTorch: 2.0.1+cu117\n",
      "/home/worker/miniconda3/lib/python3.11/site-packages/nncf/torch/dynamic_graph/wrappers.py:74: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  op1 = operator(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n",
      "Set CACHE_DIR to /tmp/tmp642hefn5/model_cache\n"
     ]
    }
   ],
   "source": [
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model_non_optimized = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "model_optimized = OVModelForSequenceClassification.from_pretrained(model_id, export=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac46572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_non_optimized():\n",
    "    inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "    outputs = model_non_optimized(**inputs)\n",
    "\n",
    "def run_inference_optimized():\n",
    "    inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "    outputs = model_optimized(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e2b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_non_optimized = benchmark.Timer(\n",
    "    stmt=\"run_inference_non_optimized()\",\n",
    "    setup=\"from __main__ import run_inference_non_optimized\",\n",
    "    num_threads=1,\n",
    ")\n",
    "\n",
    "timer_optimized = benchmark.Timer(\n",
    "    stmt=\"run_inference_optimized()\",\n",
    "    setup=\"from __main__ import run_inference_optimized\",\n",
    "    num_threads=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70576bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-optimized model: <torch.utils.benchmark.utils.common.Measurement object at 0x7ff4a0cc1c10>\n",
      "run_inference_non_optimized()\n",
      "setup: from __main__ import run_inference_non_optimized\n",
      "  54.75 ms\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "result_non_optimized = timer_non_optimized.timeit(100)\n",
    "print(\"Non-optimized model:\", result_non_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf14e031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized model: <torch.utils.benchmark.utils.common.Measurement object at 0x7ff674674850>\n",
      "run_inference_optimized()\n",
      "setup: from __main__ import run_inference_optimized\n",
      "  5.34 ms\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "result_optimized = timer_optimized.timeit(100)\n",
    "print(\"Optimized model:\", result_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77fc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for item in dataset:\n",
    "        inputs = tokenizer(item['sentence'], return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "        correct_predictions += (predicted_label == item['label'])\n",
    "        total_predictions += 1\n",
    "\n",
    "    return correct_predictions / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ebdb207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of non-optimized model: 91.0550%\n"
     ]
    }
   ],
   "source": [
    "accuracy_non_optimized = calculate_accuracy(model_non_optimized)\n",
    "print(f'Accuracy of non-optimized model: {accuracy_non_optimized * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c94acd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of optimized model: 91.0550%\n"
     ]
    }
   ],
   "source": [
    "accuracy_optimized = calculate_accuracy(model_optimized)\n",
    "print(f'Accuracy of optimized model: {accuracy_optimized * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca910054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
