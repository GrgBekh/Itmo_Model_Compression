{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa2a3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from optimum.intel import OVModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd269fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using framework PyTorch: 2.0.1+cu117\n",
      "/home/worker/miniconda3/lib/python3.11/site-packages/nncf/torch/dynamic_graph/wrappers.py:74: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  op1 = operator(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n",
      "Set CACHE_DIR to /tmp/tmpe8ahvj51/model_cache\n"
     ]
    }
   ],
   "source": [
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model_non_optimized = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "model_optimized = OVModelForSequenceClassification.from_pretrained(model_id, export=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac46572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_non_optimized():\n",
    "    inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "    outputs = model_non_optimized(**inputs)\n",
    "\n",
    "def run_inference_optimized():\n",
    "    inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "    outputs = model_optimized(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e2b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_non_optimized = benchmark.Timer(\n",
    "    stmt=\"run_inference_non_optimized()\",\n",
    "    setup=\"from __main__ import run_inference_non_optimized\",\n",
    "    num_threads=1,\n",
    ")\n",
    "\n",
    "timer_optimized = benchmark.Timer(\n",
    "    stmt=\"run_inference_optimized()\",\n",
    "    setup=\"from __main__ import run_inference_optimized\",\n",
    "    num_threads=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70576bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-optimized model: <torch.utils.benchmark.utils.common.Measurement object at 0x7f08166f4d50>\n",
      "run_inference_non_optimized()\n",
      "setup: from __main__ import run_inference_non_optimized\n",
      "  51.80 ms\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "result_non_optimized = timer_non_optimized.timeit(100)\n",
    "print(\"Non-optimized model:\", result_non_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf14e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized model: <torch.utils.benchmark.utils.common.Measurement object at 0x7f082eae54d0>\n",
      "run_inference_optimized()\n",
      "setup: from __main__ import run_inference_optimized\n",
      "  5.01 ms\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "result_optimized = timer_optimized.timeit(100)\n",
    "print(\"Optimized model:\", result_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94acd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
